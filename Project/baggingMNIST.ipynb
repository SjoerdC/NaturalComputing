{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "baggingMNIST.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fde69AMuOpox",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import mnist\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation\n",
        "from keras.models import Sequential\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qYrab7qpOppj",
        "colab": {}
      },
      "source": [
        "batch_size = 64\n",
        "epochs = 3\n",
        "IMAGE_WIDTH = 32\n",
        "IMAGE_HEIGHT = 32\n",
        "NUM_CLASSES = 10\n",
        "NUM_MODELS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8QvEt97vF52",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JtJIUBsFKeRO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(imgs):\n",
        "    \n",
        "    processed = []\n",
        "    \n",
        "    for img in tqdm(imgs):\n",
        "        processed.append(cv2.resize(img, (IMAGE_WIDTH, IMAGE_HEIGHT), interpolation = cv2.INTER_AREA))\n",
        "    \n",
        "    proccessed = np.array(processed)\n",
        "    return proccessed.reshape(proccessed.shape[0], IMAGE_WIDTH, IMAGE_HEIGHT, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XypdmBJROpp9",
        "outputId": "6abf3f21-3dfc-4779-f921-51ff3c8cbef5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = preprocess(x_train)\n",
        "x_test = preprocess(x_test)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 60000/60000 [00:00<00:00, 128836.29it/s]\n",
            "100%|██████████| 10000/10000 [00:00<00:00, 133881.42it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 32, 32, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mo8yHyg-Opqo",
        "colab": {}
      },
      "source": [
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_test = keras.utils.to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4SYRuKZaIwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIBGIrlkvOt0",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zLWph6_aOpr2",
        "colab": {}
      },
      "source": [
        "def create_model():\n",
        "    conv_base = VGG16(input_shape = (IMAGE_HEIGHT,IMAGE_WIDTH, 1),\n",
        "                            include_top = False, weights = None, classes=NUM_CLASSES)\n",
        "\n",
        "    conv_base.trainable = True\n",
        "\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(conv_base)\n",
        "    model.add(Dropout(0.2))\n",
        "    model.add(Flatten())\n",
        "    model.add(Dense(512))\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(Dense(NUM_CLASSES))\n",
        "    model.add(Activation('softmax'))\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate=0.0001)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizer,\n",
        "              metrics=['categorical_accuracy'])\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVqdcrD_vQ-Q",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HjvZqLBJOpsw",
        "outputId": "5100a2fe-d066-4011-c323-ac36a8833eaa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "models = []\n",
        "\n",
        "for i in range(NUM_MODELS):\n",
        "\n",
        "    print(f\"Train model {i}\")\n",
        "    idx = np.random.choice(len(x_train), size=len(x_train), replace=True)\n",
        "\n",
        "    x_train_model = x_train[idx]\n",
        "    y_train_model = y_train[idx]\n",
        "\n",
        "    model = create_model()\n",
        "    model.fit(x_train_model,y_train_model,\n",
        "              batch_size = batch_size,\n",
        "              epochs = epochs,\n",
        "              validation_data = (x_test,y_test),\n",
        "              shuffle = True)\n",
        "    models.append(model)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train model 0\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 19s 2ms/step - loss: 1.1204 - categorical_accuracy: 0.5968 - val_loss: 0.3516 - val_categorical_accuracy: 0.8835\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.2547 - categorical_accuracy: 0.9208 - val_loss: 0.1371 - val_categorical_accuracy: 0.9553\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.1427 - categorical_accuracy: 0.9581 - val_loss: 0.1332 - val_categorical_accuracy: 0.9601\n",
            "Train model 1\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 19s 2ms/step - loss: 1.1796 - categorical_accuracy: 0.5729 - val_loss: 0.3383 - val_categorical_accuracy: 0.8957\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.2500 - categorical_accuracy: 0.9225 - val_loss: 0.1448 - val_categorical_accuracy: 0.9547\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.1394 - categorical_accuracy: 0.9564 - val_loss: 0.1317 - val_categorical_accuracy: 0.9611\n",
            "Train model 2\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 19s 2ms/step - loss: 1.1793 - categorical_accuracy: 0.5725 - val_loss: 0.3874 - val_categorical_accuracy: 0.8732\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.2906 - categorical_accuracy: 0.9117 - val_loss: 0.1977 - val_categorical_accuracy: 0.9404\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.1515 - categorical_accuracy: 0.9513 - val_loss: 0.1164 - val_categorical_accuracy: 0.9654\n",
            "Train model 3\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 19s 2ms/step - loss: 1.1746 - categorical_accuracy: 0.5790 - val_loss: 0.3536 - val_categorical_accuracy: 0.8896\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.2697 - categorical_accuracy: 0.9148 - val_loss: 0.1817 - val_categorical_accuracy: 0.9417\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.1341 - categorical_accuracy: 0.9606 - val_loss: 0.1566 - val_categorical_accuracy: 0.9497\n",
            "Train model 4\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 19s 2ms/step - loss: 1.2026 - categorical_accuracy: 0.5563 - val_loss: 0.3601 - val_categorical_accuracy: 0.8837\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.2678 - categorical_accuracy: 0.9181 - val_loss: 0.1934 - val_categorical_accuracy: 0.9402\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.1277 - categorical_accuracy: 0.9614 - val_loss: 0.1054 - val_categorical_accuracy: 0.9676\n",
            "Train model 5\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 19s 2ms/step - loss: 1.1835 - categorical_accuracy: 0.5684 - val_loss: 0.3329 - val_categorical_accuracy: 0.8941\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.2542 - categorical_accuracy: 0.9206 - val_loss: 0.1516 - val_categorical_accuracy: 0.9539\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.1498 - categorical_accuracy: 0.9537 - val_loss: 0.1206 - val_categorical_accuracy: 0.9650\n",
            "Train model 6\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 19s 2ms/step - loss: 1.1149 - categorical_accuracy: 0.6044 - val_loss: 0.4718 - val_categorical_accuracy: 0.8373\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.2885 - categorical_accuracy: 0.9090 - val_loss: 0.1957 - val_categorical_accuracy: 0.9431\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.1581 - categorical_accuracy: 0.9530 - val_loss: 0.1545 - val_categorical_accuracy: 0.9526\n",
            "Train model 7\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 19s 2ms/step - loss: 1.3535 - categorical_accuracy: 0.5194 - val_loss: 0.4243 - val_categorical_accuracy: 0.8642\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.2972 - categorical_accuracy: 0.9071 - val_loss: 0.1632 - val_categorical_accuracy: 0.9484\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.1567 - categorical_accuracy: 0.9524 - val_loss: 0.1468 - val_categorical_accuracy: 0.9542\n",
            "Train model 8\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 19s 2ms/step - loss: 1.0556 - categorical_accuracy: 0.6298 - val_loss: 0.4252 - val_categorical_accuracy: 0.8642\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.2707 - categorical_accuracy: 0.9152 - val_loss: 0.1473 - val_categorical_accuracy: 0.9553\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.1382 - categorical_accuracy: 0.9565 - val_loss: 0.0955 - val_categorical_accuracy: 0.9693\n",
            "Train model 9\n",
            "Train on 10000 samples, validate on 10000 samples\n",
            "Epoch 1/3\n",
            "10000/10000 [==============================] - 19s 2ms/step - loss: 1.1500 - categorical_accuracy: 0.5808 - val_loss: 0.5849 - val_categorical_accuracy: 0.7935\n",
            "Epoch 2/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.2931 - categorical_accuracy: 0.9064 - val_loss: 0.2131 - val_categorical_accuracy: 0.9318\n",
            "Epoch 3/3\n",
            "10000/10000 [==============================] - 18s 2ms/step - loss: 0.1615 - categorical_accuracy: 0.9486 - val_loss: 0.0951 - val_categorical_accuracy: 0.9711\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbiuqESLvTOY",
        "colab_type": "text"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JXFkx19XmqKe",
        "colab_type": "code",
        "outputId": "02113f7a-0154-4b5c-bd26-b77b5eb47077",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "prediction = []\n",
        "\n",
        "for m in tqdm(models):\n",
        "    prediction.append(np.argmax(m.predict(x_test), axis=1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 10/10 [00:52<00:00,  5.22s/it]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Rhe8NT_nDSb",
        "colab_type": "code",
        "outputId": "c39bca85-de03-4c96-ede0-d1289d10b777",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "prediction = np.transpose(prediction)\n",
        "prediction = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=prediction)\n",
        "\n",
        "print('Test accuracy:', accuracy_score(prediction, np.argmax(y_test, axis=1)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test accuracy: 0.9769\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}