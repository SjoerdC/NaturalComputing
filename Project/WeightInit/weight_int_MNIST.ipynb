{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "weight_int_MNIST.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fde69AMuOpox",
        "outputId": "7c73ca05-0026-4139-8eeb-d124576ccbe5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import initializers\n",
        "from itertools import count\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import mnist\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, Input, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "from scipy.stats import pearsonr\n",
        "from tqdm import tqdm"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qYrab7qpOppj",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 9999\n",
        "IMAGE_SIZE = 28\n",
        "NUM_CLASSES = 10\n",
        "NUM_CHANNELS = 1\n",
        "MODEL_ADDITION_DELTA = 0.01\n",
        "MODEL_ADDITION_PATIENCE = 3\n",
        "MODEL_NAME = \"MNIST_weight_init\"\n",
        "PATH = \"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "R9M4_-IaBOsn"
      },
      "source": [
        "# Set seeds"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "7n9nJGd_BQ-r",
        "colab": {}
      },
      "source": [
        "run = \"run3\"\n",
        "np.random.seed(3)\n",
        "tf.random.set_seed(3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g8QvEt97vF52"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JtJIUBsFKeRO",
        "colab": {}
      },
      "source": [
        "def preprocess(imgs):\n",
        "    \n",
        "    return imgs.reshape(imgs.shape[0], IMAGE_SIZE, IMAGE_SIZE, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XypdmBJROpp9",
        "outputId": "258f283d-0337-4e3d-e736-76cc048118fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = preprocess(x_train)\n",
        "x_test = preprocess(x_test)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "60000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mo8yHyg-Opqo",
        "colab": {}
      },
      "source": [
        "# Convert class vectors to binary class matrices.\n",
        "y_train = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_testc = keras.utils.to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "a4SYRuKZaIwb",
        "colab": {}
      },
      "source": [
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vBci5ba9hiaQ",
        "colab": {}
      },
      "source": [
        "# Split the data\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size=0.20, shuffle= True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gIBGIrlkvOt0"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zLWph6_aOpr2",
        "colab": {}
      },
      "source": [
        "def MNISTmodel(imsize, num_classes, num_channels):\n",
        "    inputs = Input((imsize,imsize,num_channels))\n",
        "    x = Conv2D(filters = 32, kernel_size = (3,3), activation = 'relu', strides = 2)(inputs)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size = (2,2), strides=(2,2), padding = \"same\")(x)\n",
        "    x = Conv2D(filters=32, kernel_size=(1,1), activation='relu', padding='valid')(x)\n",
        "    x = Conv2D(filters = 10, kernel_size = (1,1),strides = (1,1), padding = 'valid')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    outputs = Activation('softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    optimizer = keras.optimizers.Adam(learning_rate = 1e-04)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TVqdcrD_vQ-Q"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HjvZqLBJOpsw",
        "outputId": "e46a751e-b330-43b1-faa7-233b11e42260",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "models = []\n",
        "accuracies = []\n",
        "predictions = []\n",
        "initializer = [\"Zero\",\"Ones\",\"Random Normal\",\"Random Uniform\",\"Identity\",\"Orthogonal\",\"Glorot Normal\",\"Glorot Uniform\"]\n",
        "for i in range(len(initializer)):\n",
        "\n",
        "    print(f\"Train model {i}\")\n",
        "    print(f\"Weight init method: {initializer[i]} \")\n",
        "    model = MNISTmodel(IMAGE_SIZE,NUM_CLASSES,NUM_CHANNELS)\n",
        "    \n",
        "    for layer in model.layers: \n",
        "        if hasattr(layer, 'kernel_initializer'):\n",
        "            if(initializer[i] == \"Zero\"):\n",
        "                layer.kernel_initializer = initializers.Zeros()\n",
        "            elif(initializer[i] == \"Ones\"):\n",
        "                layer.kernel_initializer = initializers.Ones()\n",
        "            elif(initializer[i] == \"Random Normal\"):\n",
        "                layer.kernel_initializer = initializers.RandomNormal()\n",
        "            elif(initializer[i] == \"Random Unifrom\"):\n",
        "                layer.kernel_initializer = initializers.RandomUniform()\n",
        "            elif(initializer[i] == \"Identity\"):\n",
        "                layer.kernel_initializer = initializers.Identity()\n",
        "            elif(initializer[i] == \"Orthogonal\"):\n",
        "                layer.kernel_initializer = initializers.Orthogonal()\n",
        "            elif(initializer[i] == \"Glorot Normal\"):\n",
        "                layer.kernel_initializer = initializers.GlorotNormal()\n",
        "            elif(initializer[i] == \"Glorot Unifrom\"):\n",
        "                layer.kernel_initializer = initializers.GlorotUnifrom()\n",
        "          \n",
        "    es = EarlyStopping(monitor='val_loss', min_delta=0.01, patience=3)\n",
        "    model.fit(x_train,y_train,\n",
        "              batch_size = BATCH_SIZE,\n",
        "              epochs = EPOCHS,\n",
        "              validation_data = (x_val,y_val),\n",
        "              shuffle = True,\n",
        "              callbacks=[es])\n",
        "    models.append(model)\n",
        "    y_prob = model.predict(x_test) \n",
        "    predictions.append(y_prob.argmax(axis=-1))\n",
        "    acc = model.evaluate(x_test,y_testc)[1]\n",
        "    accuracies.append(acc)\n",
        "\n",
        "    print(f\"Model: {i} added. Resulting score: {acc}\")\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train model 0\n",
            "Weight init method: Zero \n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/9999\n",
            "48000/48000 [==============================] - 5s 102us/step - loss: 2.2536 - accuracy: 0.1469 - val_loss: 2.2540 - val_accuracy: 0.2584\n",
            "Epoch 2/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 2.0958 - accuracy: 0.3349 - val_loss: 2.0326 - val_accuracy: 0.3536\n",
            "Epoch 3/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.9452 - accuracy: 0.3796 - val_loss: 1.8768 - val_accuracy: 0.3853\n",
            "Epoch 4/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.8181 - accuracy: 0.4025 - val_loss: 1.7635 - val_accuracy: 0.4176\n",
            "Epoch 5/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.7185 - accuracy: 0.4295 - val_loss: 1.6738 - val_accuracy: 0.4425\n",
            "Epoch 6/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.6412 - accuracy: 0.4504 - val_loss: 1.6087 - val_accuracy: 0.4646\n",
            "Epoch 7/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.5808 - accuracy: 0.4687 - val_loss: 1.5582 - val_accuracy: 0.4839\n",
            "Epoch 8/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.5324 - accuracy: 0.4844 - val_loss: 1.5129 - val_accuracy: 0.4986\n",
            "Epoch 9/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.4927 - accuracy: 0.4988 - val_loss: 1.4751 - val_accuracy: 0.5010\n",
            "Epoch 10/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.4573 - accuracy: 0.5104 - val_loss: 1.4410 - val_accuracy: 0.5195\n",
            "Epoch 11/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.4254 - accuracy: 0.5213 - val_loss: 1.4121 - val_accuracy: 0.5307\n",
            "Epoch 12/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.3959 - accuracy: 0.5327 - val_loss: 1.3833 - val_accuracy: 0.5439\n",
            "Epoch 13/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.3691 - accuracy: 0.5417 - val_loss: 1.3574 - val_accuracy: 0.5520\n",
            "Epoch 14/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3424 - accuracy: 0.5540 - val_loss: 1.3312 - val_accuracy: 0.5633\n",
            "Epoch 15/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3182 - accuracy: 0.5647 - val_loss: 1.3108 - val_accuracy: 0.5742\n",
            "Epoch 16/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2949 - accuracy: 0.5728 - val_loss: 1.2841 - val_accuracy: 0.5854\n",
            "Epoch 17/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2726 - accuracy: 0.5823 - val_loss: 1.2631 - val_accuracy: 0.5907\n",
            "Epoch 18/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2527 - accuracy: 0.5888 - val_loss: 1.2438 - val_accuracy: 0.6005\n",
            "Epoch 19/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2337 - accuracy: 0.5954 - val_loss: 1.2254 - val_accuracy: 0.6020\n",
            "Epoch 20/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 1.2150 - accuracy: 0.6018 - val_loss: 1.2087 - val_accuracy: 0.6099\n",
            "Epoch 21/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1979 - accuracy: 0.6074 - val_loss: 1.1910 - val_accuracy: 0.6123\n",
            "Epoch 22/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.1815 - accuracy: 0.6132 - val_loss: 1.1779 - val_accuracy: 0.6141\n",
            "Epoch 23/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1659 - accuracy: 0.6176 - val_loss: 1.1602 - val_accuracy: 0.6256\n",
            "Epoch 24/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1514 - accuracy: 0.6251 - val_loss: 1.1458 - val_accuracy: 0.6293\n",
            "Epoch 25/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.1371 - accuracy: 0.6296 - val_loss: 1.1335 - val_accuracy: 0.6374\n",
            "Epoch 26/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1238 - accuracy: 0.6351 - val_loss: 1.1193 - val_accuracy: 0.6385\n",
            "Epoch 27/9999\n",
            "48000/48000 [==============================] - 4s 79us/step - loss: 1.1117 - accuracy: 0.6379 - val_loss: 1.1056 - val_accuracy: 0.6425\n",
            "Epoch 28/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 1.0986 - accuracy: 0.6449 - val_loss: 1.0956 - val_accuracy: 0.6462\n",
            "Epoch 29/9999\n",
            "48000/48000 [==============================] - 4s 79us/step - loss: 1.0870 - accuracy: 0.6490 - val_loss: 1.0823 - val_accuracy: 0.6482\n",
            "Epoch 30/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 1.0759 - accuracy: 0.6528 - val_loss: 1.0811 - val_accuracy: 0.6442\n",
            "Epoch 31/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 1.0634 - accuracy: 0.6583 - val_loss: 1.0608 - val_accuracy: 0.6633\n",
            "Epoch 32/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0549 - accuracy: 0.6620 - val_loss: 1.0504 - val_accuracy: 0.6645\n",
            "Epoch 33/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0438 - accuracy: 0.6664 - val_loss: 1.0465 - val_accuracy: 0.6637\n",
            "Epoch 34/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0335 - accuracy: 0.6687 - val_loss: 1.0314 - val_accuracy: 0.6749\n",
            "Epoch 35/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0237 - accuracy: 0.6728 - val_loss: 1.0227 - val_accuracy: 0.6813\n",
            "Epoch 36/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0150 - accuracy: 0.6761 - val_loss: 1.0123 - val_accuracy: 0.6819\n",
            "Epoch 37/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0057 - accuracy: 0.6801 - val_loss: 1.0036 - val_accuracy: 0.6820\n",
            "Epoch 38/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9969 - accuracy: 0.6825 - val_loss: 1.0009 - val_accuracy: 0.6822\n",
            "Epoch 39/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9874 - accuracy: 0.6853 - val_loss: 0.9877 - val_accuracy: 0.6932\n",
            "Epoch 40/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9788 - accuracy: 0.6884 - val_loss: 0.9790 - val_accuracy: 0.6950\n",
            "Epoch 41/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 0.9711 - accuracy: 0.6915 - val_loss: 0.9713 - val_accuracy: 0.6973\n",
            "Epoch 42/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 0.9635 - accuracy: 0.6935 - val_loss: 0.9643 - val_accuracy: 0.6983\n",
            "Epoch 43/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9555 - accuracy: 0.6971 - val_loss: 0.9558 - val_accuracy: 0.7000\n",
            "Epoch 44/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9479 - accuracy: 0.6999 - val_loss: 0.9489 - val_accuracy: 0.7067\n",
            "Epoch 45/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9401 - accuracy: 0.7026 - val_loss: 0.9422 - val_accuracy: 0.7088\n",
            "Epoch 46/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9332 - accuracy: 0.7059 - val_loss: 0.9376 - val_accuracy: 0.7054\n",
            "Epoch 47/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9259 - accuracy: 0.7076 - val_loss: 0.9313 - val_accuracy: 0.7081\n",
            "Epoch 48/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9189 - accuracy: 0.7093 - val_loss: 0.9232 - val_accuracy: 0.7118\n",
            "Epoch 49/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9113 - accuracy: 0.7127 - val_loss: 0.9172 - val_accuracy: 0.7099\n",
            "Epoch 50/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9054 - accuracy: 0.7149 - val_loss: 0.9113 - val_accuracy: 0.7201\n",
            "Epoch 51/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8987 - accuracy: 0.7157 - val_loss: 0.9032 - val_accuracy: 0.7208\n",
            "Epoch 52/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 0.8942 - accuracy: 0.7184 - val_loss: 0.8979 - val_accuracy: 0.7240\n",
            "Epoch 53/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8862 - accuracy: 0.7207 - val_loss: 0.8932 - val_accuracy: 0.7255\n",
            "Epoch 54/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 0.8810 - accuracy: 0.7225 - val_loss: 0.8886 - val_accuracy: 0.7175\n",
            "Epoch 55/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8754 - accuracy: 0.7226 - val_loss: 0.8830 - val_accuracy: 0.7279\n",
            "Epoch 56/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8698 - accuracy: 0.7260 - val_loss: 0.8816 - val_accuracy: 0.7278\n",
            "Epoch 57/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8645 - accuracy: 0.7286 - val_loss: 0.8691 - val_accuracy: 0.7308\n",
            "Epoch 58/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8581 - accuracy: 0.7297 - val_loss: 0.8653 - val_accuracy: 0.7262\n",
            "Epoch 59/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8545 - accuracy: 0.7302 - val_loss: 0.8600 - val_accuracy: 0.7306\n",
            "Epoch 60/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8491 - accuracy: 0.7326 - val_loss: 0.8569 - val_accuracy: 0.7305\n",
            "Epoch 61/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8442 - accuracy: 0.7339 - val_loss: 0.8511 - val_accuracy: 0.7367\n",
            "Epoch 62/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8394 - accuracy: 0.7363 - val_loss: 0.8483 - val_accuracy: 0.7329\n",
            "Epoch 63/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8350 - accuracy: 0.7366 - val_loss: 0.8417 - val_accuracy: 0.7372\n",
            "Epoch 64/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8302 - accuracy: 0.7380 - val_loss: 0.8388 - val_accuracy: 0.7376\n",
            "Epoch 65/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8254 - accuracy: 0.7401 - val_loss: 0.8369 - val_accuracy: 0.7373\n",
            "Epoch 66/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8225 - accuracy: 0.7407 - val_loss: 0.8303 - val_accuracy: 0.7401\n",
            "Epoch 67/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8178 - accuracy: 0.7434 - val_loss: 0.8261 - val_accuracy: 0.7421\n",
            "Epoch 68/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8146 - accuracy: 0.7428 - val_loss: 0.8235 - val_accuracy: 0.7428\n",
            "Epoch 69/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8108 - accuracy: 0.7448 - val_loss: 0.8182 - val_accuracy: 0.7402\n",
            "Epoch 70/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8068 - accuracy: 0.7450 - val_loss: 0.8147 - val_accuracy: 0.7436\n",
            "Epoch 71/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8030 - accuracy: 0.7456 - val_loss: 0.8121 - val_accuracy: 0.7438\n",
            "Epoch 72/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7992 - accuracy: 0.7466 - val_loss: 0.8077 - val_accuracy: 0.7481\n",
            "Epoch 73/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7959 - accuracy: 0.7480 - val_loss: 0.8034 - val_accuracy: 0.7465\n",
            "Epoch 74/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7919 - accuracy: 0.7501 - val_loss: 0.8021 - val_accuracy: 0.7495\n",
            "Epoch 75/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7892 - accuracy: 0.7509 - val_loss: 0.7968 - val_accuracy: 0.7510\n",
            "Epoch 76/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7862 - accuracy: 0.7511 - val_loss: 0.7967 - val_accuracy: 0.7494\n",
            "Epoch 77/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7833 - accuracy: 0.7515 - val_loss: 0.7935 - val_accuracy: 0.7521\n",
            "Epoch 78/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7793 - accuracy: 0.7537 - val_loss: 0.7880 - val_accuracy: 0.7523\n",
            "10000/10000 [==============================] - 1s 95us/step\n",
            "Model: 0 added. Resulting score: 0.7627000212669373\n",
            "Train model 1\n",
            "Weight init method: Ones \n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/9999\n",
            "48000/48000 [==============================] - 4s 85us/step - loss: 2.2565 - accuracy: 0.2121 - val_loss: 2.2603 - val_accuracy: 0.2463\n",
            "Epoch 2/9999\n",
            "48000/48000 [==============================] - 4s 79us/step - loss: 2.1170 - accuracy: 0.2964 - val_loss: 2.0377 - val_accuracy: 0.3308\n",
            "Epoch 3/9999\n",
            "48000/48000 [==============================] - 4s 81us/step - loss: 1.9500 - accuracy: 0.3493 - val_loss: 1.8757 - val_accuracy: 0.3815\n",
            "Epoch 4/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.8237 - accuracy: 0.3931 - val_loss: 1.7704 - val_accuracy: 0.4098\n",
            "Epoch 5/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.7298 - accuracy: 0.4238 - val_loss: 1.6882 - val_accuracy: 0.4314\n",
            "Epoch 6/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.6555 - accuracy: 0.4462 - val_loss: 1.6223 - val_accuracy: 0.4663\n",
            "Epoch 7/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.5942 - accuracy: 0.4690 - val_loss: 1.5651 - val_accuracy: 0.4823\n",
            "Epoch 8/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.5420 - accuracy: 0.4859 - val_loss: 1.5177 - val_accuracy: 0.4955\n",
            "Epoch 9/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.4960 - accuracy: 0.5040 - val_loss: 1.4720 - val_accuracy: 0.5169\n",
            "Epoch 10/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.4547 - accuracy: 0.5176 - val_loss: 1.4338 - val_accuracy: 0.5201\n",
            "Epoch 11/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.4173 - accuracy: 0.5273 - val_loss: 1.3988 - val_accuracy: 0.5403\n",
            "Epoch 12/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3845 - accuracy: 0.5388 - val_loss: 1.3697 - val_accuracy: 0.5419\n",
            "Epoch 13/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.3543 - accuracy: 0.5450 - val_loss: 1.3382 - val_accuracy: 0.5556\n",
            "Epoch 14/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.3270 - accuracy: 0.5521 - val_loss: 1.3135 - val_accuracy: 0.5619\n",
            "Epoch 15/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.3018 - accuracy: 0.5607 - val_loss: 1.2886 - val_accuracy: 0.5652\n",
            "Epoch 16/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2803 - accuracy: 0.5664 - val_loss: 1.2673 - val_accuracy: 0.5725\n",
            "Epoch 17/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.2593 - accuracy: 0.5722 - val_loss: 1.2503 - val_accuracy: 0.5833\n",
            "Epoch 18/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 1.2402 - accuracy: 0.5804 - val_loss: 1.2294 - val_accuracy: 0.5869\n",
            "Epoch 19/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.2224 - accuracy: 0.5847 - val_loss: 1.2131 - val_accuracy: 0.5916\n",
            "Epoch 20/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2066 - accuracy: 0.5892 - val_loss: 1.1985 - val_accuracy: 0.5912\n",
            "Epoch 21/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1908 - accuracy: 0.5928 - val_loss: 1.1820 - val_accuracy: 0.5988\n",
            "Epoch 22/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1765 - accuracy: 0.5979 - val_loss: 1.1710 - val_accuracy: 0.6030\n",
            "Epoch 23/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1627 - accuracy: 0.6045 - val_loss: 1.1569 - val_accuracy: 0.6064\n",
            "Epoch 24/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1510 - accuracy: 0.6083 - val_loss: 1.1447 - val_accuracy: 0.6108\n",
            "Epoch 25/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1391 - accuracy: 0.6129 - val_loss: 1.1335 - val_accuracy: 0.6146\n",
            "Epoch 26/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1276 - accuracy: 0.6157 - val_loss: 1.1217 - val_accuracy: 0.6184\n",
            "Epoch 27/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1169 - accuracy: 0.6191 - val_loss: 1.1130 - val_accuracy: 0.6202\n",
            "Epoch 28/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1059 - accuracy: 0.6235 - val_loss: 1.1030 - val_accuracy: 0.6289\n",
            "Epoch 29/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0955 - accuracy: 0.6290 - val_loss: 1.0934 - val_accuracy: 0.6314\n",
            "Epoch 30/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0869 - accuracy: 0.6324 - val_loss: 1.0835 - val_accuracy: 0.6327\n",
            "Epoch 31/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0772 - accuracy: 0.6355 - val_loss: 1.0765 - val_accuracy: 0.6382\n",
            "Epoch 32/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0691 - accuracy: 0.6395 - val_loss: 1.0654 - val_accuracy: 0.6396\n",
            "Epoch 33/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 1.0606 - accuracy: 0.6431 - val_loss: 1.0580 - val_accuracy: 0.6410\n",
            "Epoch 34/9999\n",
            "48000/48000 [==============================] - 4s 81us/step - loss: 1.0521 - accuracy: 0.6460 - val_loss: 1.0495 - val_accuracy: 0.6432\n",
            "Epoch 35/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 1.0431 - accuracy: 0.6503 - val_loss: 1.0428 - val_accuracy: 0.6464\n",
            "Epoch 36/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0355 - accuracy: 0.6530 - val_loss: 1.0367 - val_accuracy: 0.6495\n",
            "Epoch 37/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0273 - accuracy: 0.6571 - val_loss: 1.0256 - val_accuracy: 0.6566\n",
            "Epoch 38/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0202 - accuracy: 0.6597 - val_loss: 1.0186 - val_accuracy: 0.6611\n",
            "Epoch 39/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0125 - accuracy: 0.6631 - val_loss: 1.0125 - val_accuracy: 0.6610\n",
            "Epoch 40/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0058 - accuracy: 0.6661 - val_loss: 1.0095 - val_accuracy: 0.6652\n",
            "Epoch 41/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9982 - accuracy: 0.6687 - val_loss: 0.9975 - val_accuracy: 0.6665\n",
            "Epoch 42/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9913 - accuracy: 0.6715 - val_loss: 0.9944 - val_accuracy: 0.6690\n",
            "Epoch 43/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9851 - accuracy: 0.6727 - val_loss: 0.9894 - val_accuracy: 0.6741\n",
            "Epoch 44/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9783 - accuracy: 0.6777 - val_loss: 0.9786 - val_accuracy: 0.6731\n",
            "Epoch 45/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9719 - accuracy: 0.6787 - val_loss: 0.9726 - val_accuracy: 0.6725\n",
            "Epoch 46/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9646 - accuracy: 0.6825 - val_loss: 0.9646 - val_accuracy: 0.6823\n",
            "Epoch 47/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9581 - accuracy: 0.6844 - val_loss: 0.9588 - val_accuracy: 0.6827\n",
            "Epoch 48/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9516 - accuracy: 0.6866 - val_loss: 0.9515 - val_accuracy: 0.6853\n",
            "Epoch 49/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9451 - accuracy: 0.6903 - val_loss: 0.9472 - val_accuracy: 0.6877\n",
            "Epoch 50/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9399 - accuracy: 0.6913 - val_loss: 0.9388 - val_accuracy: 0.6918\n",
            "Epoch 51/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9326 - accuracy: 0.6946 - val_loss: 0.9340 - val_accuracy: 0.6927\n",
            "Epoch 52/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9277 - accuracy: 0.6973 - val_loss: 0.9286 - val_accuracy: 0.6940\n",
            "Epoch 53/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9210 - accuracy: 0.6989 - val_loss: 0.9202 - val_accuracy: 0.6984\n",
            "Epoch 54/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9158 - accuracy: 0.7011 - val_loss: 0.9154 - val_accuracy: 0.7001\n",
            "Epoch 55/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9094 - accuracy: 0.7045 - val_loss: 0.9112 - val_accuracy: 0.7015\n",
            "Epoch 56/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9036 - accuracy: 0.7060 - val_loss: 0.9030 - val_accuracy: 0.7003\n",
            "Epoch 57/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8976 - accuracy: 0.7091 - val_loss: 0.8973 - val_accuracy: 0.7092\n",
            "Epoch 58/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8924 - accuracy: 0.7105 - val_loss: 0.8921 - val_accuracy: 0.7063\n",
            "Epoch 59/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8867 - accuracy: 0.7125 - val_loss: 0.8862 - val_accuracy: 0.7089\n",
            "Epoch 60/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8809 - accuracy: 0.7148 - val_loss: 0.8808 - val_accuracy: 0.7153\n",
            "Epoch 61/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8747 - accuracy: 0.7180 - val_loss: 0.8761 - val_accuracy: 0.7143\n",
            "Epoch 62/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.8693 - accuracy: 0.7190 - val_loss: 0.8721 - val_accuracy: 0.7172\n",
            "Epoch 63/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8635 - accuracy: 0.7217 - val_loss: 0.8675 - val_accuracy: 0.7160\n",
            "Epoch 64/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8578 - accuracy: 0.7235 - val_loss: 0.8602 - val_accuracy: 0.7216\n",
            "Epoch 65/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8529 - accuracy: 0.7269 - val_loss: 0.8536 - val_accuracy: 0.7245\n",
            "Epoch 66/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8464 - accuracy: 0.7280 - val_loss: 0.8470 - val_accuracy: 0.7247\n",
            "Epoch 67/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8413 - accuracy: 0.7304 - val_loss: 0.8457 - val_accuracy: 0.7232\n",
            "Epoch 68/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8360 - accuracy: 0.7321 - val_loss: 0.8371 - val_accuracy: 0.7297\n",
            "Epoch 69/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8306 - accuracy: 0.7335 - val_loss: 0.8314 - val_accuracy: 0.7290\n",
            "Epoch 70/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8249 - accuracy: 0.7364 - val_loss: 0.8270 - val_accuracy: 0.7304\n",
            "Epoch 71/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8199 - accuracy: 0.7377 - val_loss: 0.8218 - val_accuracy: 0.7345\n",
            "Epoch 72/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8150 - accuracy: 0.7395 - val_loss: 0.8205 - val_accuracy: 0.7347\n",
            "Epoch 73/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8093 - accuracy: 0.7423 - val_loss: 0.8109 - val_accuracy: 0.7376\n",
            "Epoch 74/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8050 - accuracy: 0.7429 - val_loss: 0.8083 - val_accuracy: 0.7387\n",
            "Epoch 75/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.8006 - accuracy: 0.7447 - val_loss: 0.8012 - val_accuracy: 0.7391\n",
            "Epoch 76/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7950 - accuracy: 0.7466 - val_loss: 0.7972 - val_accuracy: 0.7402\n",
            "Epoch 77/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7914 - accuracy: 0.7477 - val_loss: 0.7953 - val_accuracy: 0.7439\n",
            "Epoch 78/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7860 - accuracy: 0.7506 - val_loss: 0.7924 - val_accuracy: 0.7430\n",
            "Epoch 79/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7820 - accuracy: 0.7512 - val_loss: 0.7852 - val_accuracy: 0.7448\n",
            "Epoch 80/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7775 - accuracy: 0.7530 - val_loss: 0.7800 - val_accuracy: 0.7469\n",
            "Epoch 81/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7738 - accuracy: 0.7543 - val_loss: 0.7788 - val_accuracy: 0.7501\n",
            "Epoch 82/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7699 - accuracy: 0.7562 - val_loss: 0.7734 - val_accuracy: 0.7492\n",
            "Epoch 83/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7658 - accuracy: 0.7577 - val_loss: 0.7696 - val_accuracy: 0.7527\n",
            "Epoch 84/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7621 - accuracy: 0.7577 - val_loss: 0.7648 - val_accuracy: 0.7520\n",
            "Epoch 85/9999\n",
            "48000/48000 [==============================] - 4s 79us/step - loss: 0.7582 - accuracy: 0.7594 - val_loss: 0.7625 - val_accuracy: 0.7530\n",
            "Epoch 86/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 0.7547 - accuracy: 0.7609 - val_loss: 0.7573 - val_accuracy: 0.7542\n",
            "Epoch 87/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7517 - accuracy: 0.7615 - val_loss: 0.7538 - val_accuracy: 0.7582\n",
            "Epoch 88/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7473 - accuracy: 0.7625 - val_loss: 0.7506 - val_accuracy: 0.7582\n",
            "Epoch 89/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7432 - accuracy: 0.7644 - val_loss: 0.7503 - val_accuracy: 0.7588\n",
            "Epoch 90/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7406 - accuracy: 0.7671 - val_loss: 0.7463 - val_accuracy: 0.7585\n",
            "Epoch 91/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7378 - accuracy: 0.7662 - val_loss: 0.7434 - val_accuracy: 0.7618\n",
            "10000/10000 [==============================] - 1s 94us/step\n",
            "Model: 1 added. Resulting score: 0.7724000215530396\n",
            "Train model 2\n",
            "Weight init method: Random Normal \n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/9999\n",
            "48000/48000 [==============================] - 4s 82us/step - loss: 2.2480 - accuracy: 0.2026 - val_loss: 2.2505 - val_accuracy: 0.2341\n",
            "Epoch 2/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 2.1115 - accuracy: 0.2945 - val_loss: 2.0626 - val_accuracy: 0.3026\n",
            "Epoch 3/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.9918 - accuracy: 0.3334 - val_loss: 1.9306 - val_accuracy: 0.3436\n",
            "Epoch 4/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.8828 - accuracy: 0.3740 - val_loss: 1.8300 - val_accuracy: 0.3951\n",
            "Epoch 5/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.7924 - accuracy: 0.4131 - val_loss: 1.7488 - val_accuracy: 0.4168\n",
            "Epoch 6/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.7187 - accuracy: 0.4404 - val_loss: 1.6822 - val_accuracy: 0.4432\n",
            "Epoch 7/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.6566 - accuracy: 0.4619 - val_loss: 1.6250 - val_accuracy: 0.4705\n",
            "Epoch 8/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.6030 - accuracy: 0.4811 - val_loss: 1.5745 - val_accuracy: 0.4908\n",
            "Epoch 9/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.5561 - accuracy: 0.4945 - val_loss: 1.5314 - val_accuracy: 0.5024\n",
            "Epoch 10/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.5139 - accuracy: 0.5063 - val_loss: 1.4903 - val_accuracy: 0.5165\n",
            "Epoch 11/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.4750 - accuracy: 0.5189 - val_loss: 1.4531 - val_accuracy: 0.5214\n",
            "Epoch 12/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.4410 - accuracy: 0.5270 - val_loss: 1.4206 - val_accuracy: 0.5257\n",
            "Epoch 13/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 1.4081 - accuracy: 0.5369 - val_loss: 1.3911 - val_accuracy: 0.5335\n",
            "Epoch 14/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3783 - accuracy: 0.5453 - val_loss: 1.3615 - val_accuracy: 0.5452\n",
            "Epoch 15/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.3516 - accuracy: 0.5516 - val_loss: 1.3383 - val_accuracy: 0.5554\n",
            "Epoch 16/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.3267 - accuracy: 0.5592 - val_loss: 1.3132 - val_accuracy: 0.5601\n",
            "Epoch 17/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3034 - accuracy: 0.5669 - val_loss: 1.2913 - val_accuracy: 0.5685\n",
            "Epoch 18/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.2819 - accuracy: 0.5729 - val_loss: 1.2709 - val_accuracy: 0.5716\n",
            "Epoch 19/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2638 - accuracy: 0.5794 - val_loss: 1.2525 - val_accuracy: 0.5818\n",
            "Epoch 20/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 1.2446 - accuracy: 0.5853 - val_loss: 1.2353 - val_accuracy: 0.5815\n",
            "Epoch 21/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2282 - accuracy: 0.5906 - val_loss: 1.2198 - val_accuracy: 0.5904\n",
            "Epoch 22/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2117 - accuracy: 0.5954 - val_loss: 1.2034 - val_accuracy: 0.5991\n",
            "Epoch 23/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1971 - accuracy: 0.6016 - val_loss: 1.1894 - val_accuracy: 0.6039\n",
            "Epoch 24/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1820 - accuracy: 0.6059 - val_loss: 1.1751 - val_accuracy: 0.6076\n",
            "Epoch 25/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 1.1696 - accuracy: 0.6113 - val_loss: 1.1618 - val_accuracy: 0.6123\n",
            "Epoch 26/9999\n",
            "48000/48000 [==============================] - 4s 81us/step - loss: 1.1553 - accuracy: 0.6169 - val_loss: 1.1487 - val_accuracy: 0.6185\n",
            "Epoch 27/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 1.1443 - accuracy: 0.6211 - val_loss: 1.1402 - val_accuracy: 0.6194\n",
            "Epoch 28/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1323 - accuracy: 0.6267 - val_loss: 1.1266 - val_accuracy: 0.6245\n",
            "Epoch 29/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.1195 - accuracy: 0.6310 - val_loss: 1.1173 - val_accuracy: 0.6234\n",
            "Epoch 30/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1085 - accuracy: 0.6357 - val_loss: 1.1051 - val_accuracy: 0.6381\n",
            "Epoch 31/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0979 - accuracy: 0.6394 - val_loss: 1.0949 - val_accuracy: 0.6380\n",
            "Epoch 32/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0867 - accuracy: 0.6433 - val_loss: 1.0839 - val_accuracy: 0.6438\n",
            "Epoch 33/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0760 - accuracy: 0.6492 - val_loss: 1.0736 - val_accuracy: 0.6472\n",
            "Epoch 34/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0657 - accuracy: 0.6526 - val_loss: 1.0604 - val_accuracy: 0.6519\n",
            "Epoch 35/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0560 - accuracy: 0.6579 - val_loss: 1.0519 - val_accuracy: 0.6533\n",
            "Epoch 36/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0465 - accuracy: 0.6608 - val_loss: 1.0418 - val_accuracy: 0.6590\n",
            "Epoch 37/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0371 - accuracy: 0.6650 - val_loss: 1.0316 - val_accuracy: 0.6637\n",
            "Epoch 38/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0274 - accuracy: 0.6678 - val_loss: 1.0224 - val_accuracy: 0.6653\n",
            "Epoch 39/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0179 - accuracy: 0.6720 - val_loss: 1.0141 - val_accuracy: 0.6681\n",
            "Epoch 40/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0098 - accuracy: 0.6739 - val_loss: 1.0032 - val_accuracy: 0.6747\n",
            "Epoch 41/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9991 - accuracy: 0.6782 - val_loss: 0.9946 - val_accuracy: 0.6801\n",
            "Epoch 42/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9908 - accuracy: 0.6829 - val_loss: 0.9880 - val_accuracy: 0.6787\n",
            "Epoch 43/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9821 - accuracy: 0.6855 - val_loss: 0.9780 - val_accuracy: 0.6842\n",
            "Epoch 44/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9730 - accuracy: 0.6904 - val_loss: 0.9715 - val_accuracy: 0.6876\n",
            "Epoch 45/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9632 - accuracy: 0.6924 - val_loss: 0.9625 - val_accuracy: 0.6927\n",
            "Epoch 46/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9558 - accuracy: 0.6958 - val_loss: 0.9544 - val_accuracy: 0.6934\n",
            "Epoch 47/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9466 - accuracy: 0.6995 - val_loss: 0.9448 - val_accuracy: 0.6945\n",
            "Epoch 48/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9376 - accuracy: 0.7026 - val_loss: 0.9347 - val_accuracy: 0.7003\n",
            "Epoch 49/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9298 - accuracy: 0.7059 - val_loss: 0.9270 - val_accuracy: 0.7075\n",
            "Epoch 50/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9218 - accuracy: 0.7078 - val_loss: 0.9206 - val_accuracy: 0.7081\n",
            "Epoch 51/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9131 - accuracy: 0.7118 - val_loss: 0.9104 - val_accuracy: 0.7122\n",
            "Epoch 52/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9063 - accuracy: 0.7129 - val_loss: 0.9051 - val_accuracy: 0.7152\n",
            "Epoch 53/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8979 - accuracy: 0.7170 - val_loss: 0.8981 - val_accuracy: 0.7156\n",
            "Epoch 54/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8910 - accuracy: 0.7185 - val_loss: 0.8905 - val_accuracy: 0.7166\n",
            "Epoch 55/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8840 - accuracy: 0.7222 - val_loss: 0.8823 - val_accuracy: 0.7201\n",
            "Epoch 56/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8755 - accuracy: 0.7247 - val_loss: 0.8747 - val_accuracy: 0.7257\n",
            "Epoch 57/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8693 - accuracy: 0.7269 - val_loss: 0.8683 - val_accuracy: 0.7263\n",
            "Epoch 58/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8620 - accuracy: 0.7291 - val_loss: 0.8600 - val_accuracy: 0.7300\n",
            "Epoch 59/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8550 - accuracy: 0.7319 - val_loss: 0.8534 - val_accuracy: 0.7318\n",
            "Epoch 60/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8494 - accuracy: 0.7328 - val_loss: 0.8493 - val_accuracy: 0.7327\n",
            "Epoch 61/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8420 - accuracy: 0.7344 - val_loss: 0.8410 - val_accuracy: 0.7369\n",
            "Epoch 62/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8351 - accuracy: 0.7382 - val_loss: 0.8337 - val_accuracy: 0.7414\n",
            "Epoch 63/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8289 - accuracy: 0.7398 - val_loss: 0.8271 - val_accuracy: 0.7420\n",
            "Epoch 64/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8219 - accuracy: 0.7415 - val_loss: 0.8217 - val_accuracy: 0.7439\n",
            "Epoch 65/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8164 - accuracy: 0.7436 - val_loss: 0.8154 - val_accuracy: 0.7443\n",
            "Epoch 66/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8110 - accuracy: 0.7454 - val_loss: 0.8097 - val_accuracy: 0.7492\n",
            "Epoch 67/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8050 - accuracy: 0.7468 - val_loss: 0.8043 - val_accuracy: 0.7490\n",
            "Epoch 68/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7995 - accuracy: 0.7494 - val_loss: 0.7982 - val_accuracy: 0.7516\n",
            "Epoch 69/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7944 - accuracy: 0.7502 - val_loss: 0.7931 - val_accuracy: 0.7534\n",
            "Epoch 70/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7888 - accuracy: 0.7524 - val_loss: 0.7885 - val_accuracy: 0.7563\n",
            "Epoch 71/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7835 - accuracy: 0.7540 - val_loss: 0.7841 - val_accuracy: 0.7564\n",
            "Epoch 72/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7779 - accuracy: 0.7545 - val_loss: 0.7786 - val_accuracy: 0.7592\n",
            "Epoch 73/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7733 - accuracy: 0.7572 - val_loss: 0.7737 - val_accuracy: 0.7607\n",
            "Epoch 74/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7685 - accuracy: 0.7576 - val_loss: 0.7699 - val_accuracy: 0.7644\n",
            "Epoch 75/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7647 - accuracy: 0.7582 - val_loss: 0.7659 - val_accuracy: 0.7620\n",
            "Epoch 76/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.7599 - accuracy: 0.7606 - val_loss: 0.7617 - val_accuracy: 0.7642\n",
            "Epoch 77/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 0.7545 - accuracy: 0.7621 - val_loss: 0.7548 - val_accuracy: 0.7669\n",
            "Epoch 78/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7503 - accuracy: 0.7643 - val_loss: 0.7530 - val_accuracy: 0.7668\n",
            "Epoch 79/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7468 - accuracy: 0.7658 - val_loss: 0.7474 - val_accuracy: 0.7705\n",
            "Epoch 80/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7421 - accuracy: 0.7662 - val_loss: 0.7428 - val_accuracy: 0.7707\n",
            "Epoch 81/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7375 - accuracy: 0.7671 - val_loss: 0.7400 - val_accuracy: 0.7717\n",
            "Epoch 82/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7346 - accuracy: 0.7683 - val_loss: 0.7377 - val_accuracy: 0.7727\n",
            "Epoch 83/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7304 - accuracy: 0.7703 - val_loss: 0.7321 - val_accuracy: 0.7738\n",
            "Epoch 84/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7262 - accuracy: 0.7706 - val_loss: 0.7313 - val_accuracy: 0.7734\n",
            "Epoch 85/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7223 - accuracy: 0.7724 - val_loss: 0.7269 - val_accuracy: 0.7754\n",
            "Epoch 86/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7189 - accuracy: 0.7736 - val_loss: 0.7222 - val_accuracy: 0.7790\n",
            "10000/10000 [==============================] - 1s 95us/step\n",
            "Model: 2 added. Resulting score: 0.7836999893188477\n",
            "Train model 3\n",
            "Weight init method: Random Uniform \n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/9999\n",
            "48000/48000 [==============================] - 4s 83us/step - loss: 2.2775 - accuracy: 0.1803 - val_loss: 2.2591 - val_accuracy: 0.2024\n",
            "Epoch 2/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 2.1487 - accuracy: 0.2211 - val_loss: 2.0976 - val_accuracy: 0.2713\n",
            "Epoch 3/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 2.0302 - accuracy: 0.2874 - val_loss: 1.9641 - val_accuracy: 0.3229\n",
            "Epoch 4/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.9158 - accuracy: 0.3559 - val_loss: 1.8593 - val_accuracy: 0.3800\n",
            "Epoch 5/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.8161 - accuracy: 0.4050 - val_loss: 1.7644 - val_accuracy: 0.4279\n",
            "Epoch 6/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.7289 - accuracy: 0.4356 - val_loss: 1.6841 - val_accuracy: 0.4481\n",
            "Epoch 7/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.6535 - accuracy: 0.4598 - val_loss: 1.6139 - val_accuracy: 0.4788\n",
            "Epoch 8/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.5893 - accuracy: 0.4794 - val_loss: 1.5557 - val_accuracy: 0.4918\n",
            "Epoch 9/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.5328 - accuracy: 0.4969 - val_loss: 1.5045 - val_accuracy: 0.5035\n",
            "Epoch 10/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.4835 - accuracy: 0.5095 - val_loss: 1.4594 - val_accuracy: 0.5161\n",
            "Epoch 11/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.4413 - accuracy: 0.5211 - val_loss: 1.4203 - val_accuracy: 0.5294\n",
            "Epoch 12/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.4018 - accuracy: 0.5321 - val_loss: 1.3843 - val_accuracy: 0.5412\n",
            "Epoch 13/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.3676 - accuracy: 0.5403 - val_loss: 1.3527 - val_accuracy: 0.5409\n",
            "Epoch 14/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.3367 - accuracy: 0.5480 - val_loss: 1.3242 - val_accuracy: 0.5552\n",
            "Epoch 15/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3091 - accuracy: 0.5551 - val_loss: 1.2971 - val_accuracy: 0.5596\n",
            "Epoch 16/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2847 - accuracy: 0.5607 - val_loss: 1.2743 - val_accuracy: 0.5718\n",
            "Epoch 17/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2623 - accuracy: 0.5680 - val_loss: 1.2526 - val_accuracy: 0.5718\n",
            "Epoch 18/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2419 - accuracy: 0.5730 - val_loss: 1.2343 - val_accuracy: 0.5833\n",
            "Epoch 19/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2237 - accuracy: 0.5812 - val_loss: 1.2175 - val_accuracy: 0.5846\n",
            "Epoch 20/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2067 - accuracy: 0.5844 - val_loss: 1.2022 - val_accuracy: 0.5895\n",
            "Epoch 21/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1914 - accuracy: 0.5900 - val_loss: 1.1866 - val_accuracy: 0.5940\n",
            "Epoch 22/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.1782 - accuracy: 0.5955 - val_loss: 1.1745 - val_accuracy: 0.6021\n",
            "Epoch 23/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 1.1636 - accuracy: 0.6019 - val_loss: 1.1600 - val_accuracy: 0.6083\n",
            "Epoch 24/9999\n",
            "48000/48000 [==============================] - 4s 81us/step - loss: 1.1517 - accuracy: 0.6062 - val_loss: 1.1511 - val_accuracy: 0.6067\n",
            "Epoch 25/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 1.1399 - accuracy: 0.6105 - val_loss: 1.1381 - val_accuracy: 0.6111\n",
            "Epoch 26/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1288 - accuracy: 0.6141 - val_loss: 1.1272 - val_accuracy: 0.6160\n",
            "Epoch 27/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1174 - accuracy: 0.6209 - val_loss: 1.1177 - val_accuracy: 0.6196\n",
            "Epoch 28/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1076 - accuracy: 0.6228 - val_loss: 1.1091 - val_accuracy: 0.6280\n",
            "Epoch 29/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0977 - accuracy: 0.6282 - val_loss: 1.0973 - val_accuracy: 0.6280\n",
            "Epoch 30/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0888 - accuracy: 0.6309 - val_loss: 1.0881 - val_accuracy: 0.6352\n",
            "Epoch 31/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0798 - accuracy: 0.6371 - val_loss: 1.0810 - val_accuracy: 0.6356\n",
            "Epoch 32/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0709 - accuracy: 0.6401 - val_loss: 1.0712 - val_accuracy: 0.6416\n",
            "Epoch 33/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0619 - accuracy: 0.6431 - val_loss: 1.0647 - val_accuracy: 0.6451\n",
            "Epoch 34/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0546 - accuracy: 0.6482 - val_loss: 1.0548 - val_accuracy: 0.6465\n",
            "Epoch 35/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0453 - accuracy: 0.6506 - val_loss: 1.0489 - val_accuracy: 0.6479\n",
            "Epoch 36/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0376 - accuracy: 0.6542 - val_loss: 1.0400 - val_accuracy: 0.6552\n",
            "Epoch 37/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0295 - accuracy: 0.6569 - val_loss: 1.0315 - val_accuracy: 0.6572\n",
            "Epoch 38/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 1.0231 - accuracy: 0.6594 - val_loss: 1.0268 - val_accuracy: 0.6593\n",
            "Epoch 39/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0150 - accuracy: 0.6629 - val_loss: 1.0207 - val_accuracy: 0.6637\n",
            "Epoch 40/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0088 - accuracy: 0.6661 - val_loss: 1.0109 - val_accuracy: 0.6635\n",
            "Epoch 41/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0017 - accuracy: 0.6688 - val_loss: 1.0041 - val_accuracy: 0.6665\n",
            "Epoch 42/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9952 - accuracy: 0.6712 - val_loss: 0.9969 - val_accuracy: 0.6705\n",
            "Epoch 43/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.9874 - accuracy: 0.6741 - val_loss: 0.9932 - val_accuracy: 0.6749\n",
            "Epoch 44/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.9814 - accuracy: 0.6772 - val_loss: 0.9883 - val_accuracy: 0.6771\n",
            "Epoch 45/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9750 - accuracy: 0.6798 - val_loss: 0.9781 - val_accuracy: 0.6812\n",
            "Epoch 46/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9683 - accuracy: 0.6824 - val_loss: 0.9720 - val_accuracy: 0.6800\n",
            "Epoch 47/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9632 - accuracy: 0.6857 - val_loss: 0.9689 - val_accuracy: 0.6816\n",
            "Epoch 48/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.9571 - accuracy: 0.6875 - val_loss: 0.9619 - val_accuracy: 0.6833\n",
            "Epoch 49/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9505 - accuracy: 0.6897 - val_loss: 0.9552 - val_accuracy: 0.6864\n",
            "Epoch 50/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9446 - accuracy: 0.6933 - val_loss: 0.9484 - val_accuracy: 0.6908\n",
            "Epoch 51/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9391 - accuracy: 0.6948 - val_loss: 0.9454 - val_accuracy: 0.6925\n",
            "Epoch 52/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9343 - accuracy: 0.6957 - val_loss: 0.9374 - val_accuracy: 0.6943\n",
            "Epoch 53/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9280 - accuracy: 0.6992 - val_loss: 0.9316 - val_accuracy: 0.6960\n",
            "Epoch 54/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9222 - accuracy: 0.6995 - val_loss: 0.9269 - val_accuracy: 0.6987\n",
            "Epoch 55/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9167 - accuracy: 0.7033 - val_loss: 0.9213 - val_accuracy: 0.7008\n",
            "Epoch 56/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9111 - accuracy: 0.7051 - val_loss: 0.9172 - val_accuracy: 0.7007\n",
            "Epoch 57/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9052 - accuracy: 0.7081 - val_loss: 0.9132 - val_accuracy: 0.7022\n",
            "Epoch 58/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9007 - accuracy: 0.7092 - val_loss: 0.9064 - val_accuracy: 0.7063\n",
            "Epoch 59/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8950 - accuracy: 0.7103 - val_loss: 0.9009 - val_accuracy: 0.7060\n",
            "Epoch 60/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8903 - accuracy: 0.7120 - val_loss: 0.8984 - val_accuracy: 0.7097\n",
            "Epoch 61/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8857 - accuracy: 0.7140 - val_loss: 0.8905 - val_accuracy: 0.7113\n",
            "Epoch 62/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8803 - accuracy: 0.7181 - val_loss: 0.8873 - val_accuracy: 0.7107\n",
            "Epoch 63/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8749 - accuracy: 0.7194 - val_loss: 0.8809 - val_accuracy: 0.7139\n",
            "Epoch 64/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8700 - accuracy: 0.7207 - val_loss: 0.8757 - val_accuracy: 0.7176\n",
            "Epoch 65/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8655 - accuracy: 0.7230 - val_loss: 0.8710 - val_accuracy: 0.7178\n",
            "Epoch 66/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8603 - accuracy: 0.7231 - val_loss: 0.8759 - val_accuracy: 0.7156\n",
            "Epoch 67/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8556 - accuracy: 0.7260 - val_loss: 0.8612 - val_accuracy: 0.7211\n",
            "Epoch 68/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8504 - accuracy: 0.7284 - val_loss: 0.8566 - val_accuracy: 0.7229\n",
            "Epoch 69/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8458 - accuracy: 0.7295 - val_loss: 0.8543 - val_accuracy: 0.7240\n",
            "Epoch 70/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8413 - accuracy: 0.7313 - val_loss: 0.8483 - val_accuracy: 0.7262\n",
            "Epoch 71/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.8382 - accuracy: 0.7323 - val_loss: 0.8477 - val_accuracy: 0.7248\n",
            "Epoch 72/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8322 - accuracy: 0.7346 - val_loss: 0.8394 - val_accuracy: 0.7289\n",
            "Epoch 73/9999\n",
            "48000/48000 [==============================] - 4s 79us/step - loss: 0.8275 - accuracy: 0.7350 - val_loss: 0.8345 - val_accuracy: 0.7322\n",
            "Epoch 74/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8234 - accuracy: 0.7377 - val_loss: 0.8315 - val_accuracy: 0.7308\n",
            "Epoch 75/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8186 - accuracy: 0.7395 - val_loss: 0.8256 - val_accuracy: 0.7338\n",
            "Epoch 76/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8147 - accuracy: 0.7402 - val_loss: 0.8214 - val_accuracy: 0.7357\n",
            "Epoch 77/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8103 - accuracy: 0.7419 - val_loss: 0.8196 - val_accuracy: 0.7348\n",
            "Epoch 78/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8054 - accuracy: 0.7436 - val_loss: 0.8145 - val_accuracy: 0.7368\n",
            "Epoch 79/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8011 - accuracy: 0.7451 - val_loss: 0.8090 - val_accuracy: 0.7400\n",
            "Epoch 80/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7978 - accuracy: 0.7458 - val_loss: 0.8063 - val_accuracy: 0.7402\n",
            "Epoch 81/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7926 - accuracy: 0.7488 - val_loss: 0.8028 - val_accuracy: 0.7429\n",
            "Epoch 82/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7891 - accuracy: 0.7489 - val_loss: 0.8015 - val_accuracy: 0.7442\n",
            "10000/10000 [==============================] - 1s 91us/step\n",
            "Model: 3 added. Resulting score: 0.7620999813079834\n",
            "Train model 4\n",
            "Weight init method: Identity \n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/9999\n",
            "48000/48000 [==============================] - 4s 83us/step - loss: 2.2294 - accuracy: 0.2303 - val_loss: 2.2411 - val_accuracy: 0.2757\n",
            "Epoch 2/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 2.0987 - accuracy: 0.3103 - val_loss: 2.0462 - val_accuracy: 0.3598\n",
            "Epoch 3/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.9742 - accuracy: 0.3541 - val_loss: 1.9085 - val_accuracy: 0.3772\n",
            "Epoch 4/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.8600 - accuracy: 0.3854 - val_loss: 1.8014 - val_accuracy: 0.3934\n",
            "Epoch 5/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.7613 - accuracy: 0.4120 - val_loss: 1.7119 - val_accuracy: 0.4218\n",
            "Epoch 6/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.6797 - accuracy: 0.4354 - val_loss: 1.6378 - val_accuracy: 0.4544\n",
            "Epoch 7/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.6097 - accuracy: 0.4572 - val_loss: 1.5741 - val_accuracy: 0.4758\n",
            "Epoch 8/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.5512 - accuracy: 0.4798 - val_loss: 1.5212 - val_accuracy: 0.4918\n",
            "Epoch 9/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.5010 - accuracy: 0.4968 - val_loss: 1.4751 - val_accuracy: 0.5031\n",
            "Epoch 10/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.4580 - accuracy: 0.5086 - val_loss: 1.4351 - val_accuracy: 0.5229\n",
            "Epoch 11/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.4205 - accuracy: 0.5204 - val_loss: 1.4030 - val_accuracy: 0.5231\n",
            "Epoch 12/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.3883 - accuracy: 0.5301 - val_loss: 1.3706 - val_accuracy: 0.5344\n",
            "Epoch 13/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.3588 - accuracy: 0.5380 - val_loss: 1.3444 - val_accuracy: 0.5411\n",
            "Epoch 14/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3325 - accuracy: 0.5453 - val_loss: 1.3200 - val_accuracy: 0.5518\n",
            "Epoch 15/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.3083 - accuracy: 0.5530 - val_loss: 1.2959 - val_accuracy: 0.5550\n",
            "Epoch 16/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2867 - accuracy: 0.5595 - val_loss: 1.2758 - val_accuracy: 0.5634\n",
            "Epoch 17/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2664 - accuracy: 0.5668 - val_loss: 1.2587 - val_accuracy: 0.5696\n",
            "Epoch 18/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2476 - accuracy: 0.5701 - val_loss: 1.2383 - val_accuracy: 0.5711\n",
            "Epoch 19/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2306 - accuracy: 0.5766 - val_loss: 1.2225 - val_accuracy: 0.5795\n",
            "Epoch 20/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.2144 - accuracy: 0.5818 - val_loss: 1.2071 - val_accuracy: 0.5867\n",
            "Epoch 21/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2001 - accuracy: 0.5877 - val_loss: 1.1926 - val_accuracy: 0.5929\n",
            "Epoch 22/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1852 - accuracy: 0.5939 - val_loss: 1.1791 - val_accuracy: 0.5937\n",
            "Epoch 23/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1722 - accuracy: 0.5969 - val_loss: 1.1661 - val_accuracy: 0.6000\n",
            "Epoch 24/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.1600 - accuracy: 0.6022 - val_loss: 1.1528 - val_accuracy: 0.6044\n",
            "Epoch 25/9999\n",
            "48000/48000 [==============================] - 4s 79us/step - loss: 1.1479 - accuracy: 0.6075 - val_loss: 1.1418 - val_accuracy: 0.6106\n",
            "Epoch 26/9999\n",
            "48000/48000 [==============================] - 4s 79us/step - loss: 1.1374 - accuracy: 0.6105 - val_loss: 1.1305 - val_accuracy: 0.6146\n",
            "Epoch 27/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 1.1263 - accuracy: 0.6143 - val_loss: 1.1243 - val_accuracy: 0.6157\n",
            "Epoch 28/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1154 - accuracy: 0.6197 - val_loss: 1.1102 - val_accuracy: 0.6213\n",
            "Epoch 29/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1061 - accuracy: 0.6229 - val_loss: 1.1003 - val_accuracy: 0.6248\n",
            "Epoch 30/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0969 - accuracy: 0.6264 - val_loss: 1.0913 - val_accuracy: 0.6266\n",
            "Epoch 31/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0873 - accuracy: 0.6309 - val_loss: 1.0846 - val_accuracy: 0.6287\n",
            "Epoch 32/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0787 - accuracy: 0.6330 - val_loss: 1.0732 - val_accuracy: 0.6355\n",
            "Epoch 33/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0699 - accuracy: 0.6378 - val_loss: 1.0675 - val_accuracy: 0.6338\n",
            "Epoch 34/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0617 - accuracy: 0.6414 - val_loss: 1.0587 - val_accuracy: 0.6397\n",
            "Epoch 35/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0539 - accuracy: 0.6437 - val_loss: 1.0525 - val_accuracy: 0.6407\n",
            "Epoch 36/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0467 - accuracy: 0.6462 - val_loss: 1.0437 - val_accuracy: 0.6448\n",
            "Epoch 37/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0387 - accuracy: 0.6488 - val_loss: 1.0378 - val_accuracy: 0.6507\n",
            "Epoch 38/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0319 - accuracy: 0.6530 - val_loss: 1.0288 - val_accuracy: 0.6514\n",
            "Epoch 39/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0249 - accuracy: 0.6553 - val_loss: 1.0222 - val_accuracy: 0.6567\n",
            "Epoch 40/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0179 - accuracy: 0.6582 - val_loss: 1.0149 - val_accuracy: 0.6582\n",
            "Epoch 41/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0109 - accuracy: 0.6613 - val_loss: 1.0095 - val_accuracy: 0.6608\n",
            "Epoch 42/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0052 - accuracy: 0.6631 - val_loss: 1.0015 - val_accuracy: 0.6641\n",
            "Epoch 43/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9993 - accuracy: 0.6661 - val_loss: 0.9971 - val_accuracy: 0.6647\n",
            "Epoch 44/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9932 - accuracy: 0.6670 - val_loss: 0.9919 - val_accuracy: 0.6678\n",
            "Epoch 45/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9867 - accuracy: 0.6706 - val_loss: 0.9842 - val_accuracy: 0.6700\n",
            "Epoch 46/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9813 - accuracy: 0.6717 - val_loss: 0.9779 - val_accuracy: 0.6721\n",
            "Epoch 47/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9749 - accuracy: 0.6756 - val_loss: 0.9752 - val_accuracy: 0.6700\n",
            "Epoch 48/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9697 - accuracy: 0.6763 - val_loss: 0.9689 - val_accuracy: 0.6733\n",
            "Epoch 49/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9646 - accuracy: 0.6776 - val_loss: 0.9637 - val_accuracy: 0.6766\n",
            "Epoch 50/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9590 - accuracy: 0.6807 - val_loss: 0.9601 - val_accuracy: 0.6784\n",
            "Epoch 51/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9536 - accuracy: 0.6836 - val_loss: 0.9519 - val_accuracy: 0.6799\n",
            "Epoch 52/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9495 - accuracy: 0.6854 - val_loss: 0.9471 - val_accuracy: 0.6815\n",
            "Epoch 53/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.9443 - accuracy: 0.6868 - val_loss: 0.9428 - val_accuracy: 0.6852\n",
            "Epoch 54/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.9401 - accuracy: 0.6883 - val_loss: 0.9377 - val_accuracy: 0.6861\n",
            "Epoch 55/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.9344 - accuracy: 0.6920 - val_loss: 0.9358 - val_accuracy: 0.6871\n",
            "Epoch 56/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.9306 - accuracy: 0.6919 - val_loss: 0.9308 - val_accuracy: 0.6880\n",
            "Epoch 57/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9253 - accuracy: 0.6934 - val_loss: 0.9264 - val_accuracy: 0.6898\n",
            "Epoch 58/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.9206 - accuracy: 0.6955 - val_loss: 0.9212 - val_accuracy: 0.6927\n",
            "Epoch 59/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9171 - accuracy: 0.6978 - val_loss: 0.9182 - val_accuracy: 0.6952\n",
            "Epoch 60/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9120 - accuracy: 0.6988 - val_loss: 0.9115 - val_accuracy: 0.6975\n",
            "Epoch 61/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9084 - accuracy: 0.7006 - val_loss: 0.9100 - val_accuracy: 0.6957\n",
            "Epoch 62/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9040 - accuracy: 0.7023 - val_loss: 0.9041 - val_accuracy: 0.7015\n",
            "Epoch 63/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8989 - accuracy: 0.7044 - val_loss: 0.9005 - val_accuracy: 0.7018\n",
            "Epoch 64/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8959 - accuracy: 0.7067 - val_loss: 0.8966 - val_accuracy: 0.7031\n",
            "Epoch 65/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.8916 - accuracy: 0.7083 - val_loss: 0.9026 - val_accuracy: 0.6977\n",
            "Epoch 66/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8877 - accuracy: 0.7095 - val_loss: 0.8881 - val_accuracy: 0.7043\n",
            "Epoch 67/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8832 - accuracy: 0.7114 - val_loss: 0.8840 - val_accuracy: 0.7065\n",
            "Epoch 68/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8783 - accuracy: 0.7125 - val_loss: 0.8802 - val_accuracy: 0.7085\n",
            "Epoch 69/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8745 - accuracy: 0.7154 - val_loss: 0.8754 - val_accuracy: 0.7118\n",
            "Epoch 70/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8705 - accuracy: 0.7170 - val_loss: 0.8722 - val_accuracy: 0.7113\n",
            "Epoch 71/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8662 - accuracy: 0.7190 - val_loss: 0.8711 - val_accuracy: 0.7115\n",
            "Epoch 72/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8625 - accuracy: 0.7204 - val_loss: 0.8647 - val_accuracy: 0.7127\n",
            "Epoch 73/9999\n",
            "48000/48000 [==============================] - 4s 81us/step - loss: 0.8587 - accuracy: 0.7206 - val_loss: 0.8619 - val_accuracy: 0.7147\n",
            "Epoch 74/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.8539 - accuracy: 0.7225 - val_loss: 0.8573 - val_accuracy: 0.7197\n",
            "Epoch 75/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8490 - accuracy: 0.7260 - val_loss: 0.8531 - val_accuracy: 0.7209\n",
            "Epoch 76/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8459 - accuracy: 0.7276 - val_loss: 0.8484 - val_accuracy: 0.7218\n",
            "Epoch 77/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.8421 - accuracy: 0.7283 - val_loss: 0.8459 - val_accuracy: 0.7237\n",
            "Epoch 78/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8373 - accuracy: 0.7309 - val_loss: 0.8400 - val_accuracy: 0.7258\n",
            "Epoch 79/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8332 - accuracy: 0.7330 - val_loss: 0.8383 - val_accuracy: 0.7258\n",
            "Epoch 80/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8304 - accuracy: 0.7342 - val_loss: 0.8338 - val_accuracy: 0.7287\n",
            "Epoch 81/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.8257 - accuracy: 0.7361 - val_loss: 0.8287 - val_accuracy: 0.7289\n",
            "Epoch 82/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8217 - accuracy: 0.7383 - val_loss: 0.8254 - val_accuracy: 0.7300\n",
            "Epoch 83/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8185 - accuracy: 0.7390 - val_loss: 0.8235 - val_accuracy: 0.7326\n",
            "Epoch 84/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8132 - accuracy: 0.7397 - val_loss: 0.8177 - val_accuracy: 0.7358\n",
            "Epoch 85/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.8098 - accuracy: 0.7405 - val_loss: 0.8167 - val_accuracy: 0.7318\n",
            "Epoch 86/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8061 - accuracy: 0.7426 - val_loss: 0.8109 - val_accuracy: 0.7356\n",
            "Epoch 87/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8025 - accuracy: 0.7442 - val_loss: 0.8075 - val_accuracy: 0.7367\n",
            "Epoch 88/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7993 - accuracy: 0.7464 - val_loss: 0.8039 - val_accuracy: 0.7421\n",
            "Epoch 89/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7958 - accuracy: 0.7467 - val_loss: 0.7998 - val_accuracy: 0.7414\n",
            "Epoch 90/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7918 - accuracy: 0.7481 - val_loss: 0.7960 - val_accuracy: 0.7425\n",
            "Epoch 91/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7885 - accuracy: 0.7487 - val_loss: 0.7936 - val_accuracy: 0.7414\n",
            "Epoch 92/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7844 - accuracy: 0.7495 - val_loss: 0.7895 - val_accuracy: 0.7447\n",
            "Epoch 93/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7816 - accuracy: 0.7522 - val_loss: 0.7873 - val_accuracy: 0.7483\n",
            "10000/10000 [==============================] - 1s 93us/step\n",
            "Model: 4 added. Resulting score: 0.7610999941825867\n",
            "Train model 5\n",
            "Weight init method: Orthogonal \n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/9999\n",
            "48000/48000 [==============================] - 4s 82us/step - loss: 2.2878 - accuracy: 0.1862 - val_loss: 2.2671 - val_accuracy: 0.2246\n",
            "Epoch 2/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 2.1707 - accuracy: 0.2545 - val_loss: 2.1295 - val_accuracy: 0.3080\n",
            "Epoch 3/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 2.0607 - accuracy: 0.3092 - val_loss: 1.9924 - val_accuracy: 0.3530\n",
            "Epoch 4/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.9317 - accuracy: 0.3716 - val_loss: 1.8597 - val_accuracy: 0.4114\n",
            "Epoch 5/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.8023 - accuracy: 0.4264 - val_loss: 1.7344 - val_accuracy: 0.4498\n",
            "Epoch 6/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.6903 - accuracy: 0.4673 - val_loss: 1.6352 - val_accuracy: 0.4828\n",
            "Epoch 7/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.6004 - accuracy: 0.4959 - val_loss: 1.5558 - val_accuracy: 0.5037\n",
            "Epoch 8/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.5287 - accuracy: 0.5185 - val_loss: 1.4922 - val_accuracy: 0.5284\n",
            "Epoch 9/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.4691 - accuracy: 0.5362 - val_loss: 1.4405 - val_accuracy: 0.5317\n",
            "Epoch 10/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 1.4191 - accuracy: 0.5482 - val_loss: 1.3931 - val_accuracy: 0.5623\n",
            "Epoch 11/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.3769 - accuracy: 0.5602 - val_loss: 1.3528 - val_accuracy: 0.5717\n",
            "Epoch 12/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3390 - accuracy: 0.5689 - val_loss: 1.3197 - val_accuracy: 0.5816\n",
            "Epoch 13/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.3060 - accuracy: 0.5788 - val_loss: 1.2877 - val_accuracy: 0.5849\n",
            "Epoch 14/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2769 - accuracy: 0.5845 - val_loss: 1.2607 - val_accuracy: 0.5923\n",
            "Epoch 15/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 1.2505 - accuracy: 0.5915 - val_loss: 1.2361 - val_accuracy: 0.5972\n",
            "Epoch 16/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 1.2270 - accuracy: 0.5987 - val_loss: 1.2157 - val_accuracy: 0.5979\n",
            "Epoch 17/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 1.2048 - accuracy: 0.6034 - val_loss: 1.1939 - val_accuracy: 0.6093\n",
            "Epoch 18/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1855 - accuracy: 0.6090 - val_loss: 1.1734 - val_accuracy: 0.6137\n",
            "Epoch 19/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.1671 - accuracy: 0.6137 - val_loss: 1.1563 - val_accuracy: 0.6202\n",
            "Epoch 20/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.1501 - accuracy: 0.6180 - val_loss: 1.1410 - val_accuracy: 0.6218\n",
            "Epoch 21/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 1.1333 - accuracy: 0.6239 - val_loss: 1.1272 - val_accuracy: 0.6290\n",
            "Epoch 22/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.1185 - accuracy: 0.6298 - val_loss: 1.1113 - val_accuracy: 0.6337\n",
            "Epoch 23/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.1045 - accuracy: 0.6354 - val_loss: 1.0977 - val_accuracy: 0.6359\n",
            "Epoch 24/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0902 - accuracy: 0.6378 - val_loss: 1.0832 - val_accuracy: 0.6398\n",
            "Epoch 25/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0773 - accuracy: 0.6436 - val_loss: 1.0739 - val_accuracy: 0.6466\n",
            "Epoch 26/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0650 - accuracy: 0.6485 - val_loss: 1.0590 - val_accuracy: 0.6504\n",
            "Epoch 27/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0540 - accuracy: 0.6530 - val_loss: 1.0475 - val_accuracy: 0.6553\n",
            "Epoch 28/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0420 - accuracy: 0.6576 - val_loss: 1.0377 - val_accuracy: 0.6548\n",
            "Epoch 29/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0311 - accuracy: 0.6613 - val_loss: 1.0280 - val_accuracy: 0.6593\n",
            "Epoch 30/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0194 - accuracy: 0.6636 - val_loss: 1.0161 - val_accuracy: 0.6669\n",
            "Epoch 31/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0105 - accuracy: 0.6681 - val_loss: 1.0075 - val_accuracy: 0.6699\n",
            "Epoch 32/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0009 - accuracy: 0.6720 - val_loss: 0.9964 - val_accuracy: 0.6765\n",
            "Epoch 33/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9918 - accuracy: 0.6762 - val_loss: 0.9899 - val_accuracy: 0.6783\n",
            "Epoch 34/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9819 - accuracy: 0.6803 - val_loss: 0.9794 - val_accuracy: 0.6846\n",
            "Epoch 35/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 0.9732 - accuracy: 0.6836 - val_loss: 0.9719 - val_accuracy: 0.6851\n",
            "Epoch 36/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9656 - accuracy: 0.6862 - val_loss: 0.9626 - val_accuracy: 0.6862\n",
            "Epoch 37/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9566 - accuracy: 0.6885 - val_loss: 0.9544 - val_accuracy: 0.6903\n",
            "Epoch 38/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9482 - accuracy: 0.6930 - val_loss: 0.9479 - val_accuracy: 0.6917\n",
            "Epoch 39/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9413 - accuracy: 0.6947 - val_loss: 0.9386 - val_accuracy: 0.6946\n",
            "Epoch 40/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9329 - accuracy: 0.6979 - val_loss: 0.9333 - val_accuracy: 0.6992\n",
            "Epoch 41/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9260 - accuracy: 0.7004 - val_loss: 0.9303 - val_accuracy: 0.6957\n",
            "Epoch 42/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9182 - accuracy: 0.7041 - val_loss: 0.9178 - val_accuracy: 0.7016\n",
            "Epoch 43/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9113 - accuracy: 0.7053 - val_loss: 0.9104 - val_accuracy: 0.7048\n",
            "Epoch 44/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9052 - accuracy: 0.7080 - val_loss: 0.9032 - val_accuracy: 0.7079\n",
            "Epoch 45/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8985 - accuracy: 0.7092 - val_loss: 0.8997 - val_accuracy: 0.7109\n",
            "Epoch 46/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 0.8922 - accuracy: 0.7126 - val_loss: 0.8931 - val_accuracy: 0.7134\n",
            "Epoch 47/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8855 - accuracy: 0.7139 - val_loss: 0.8864 - val_accuracy: 0.7166\n",
            "Epoch 48/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8803 - accuracy: 0.7160 - val_loss: 0.8795 - val_accuracy: 0.7178\n",
            "Epoch 49/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8738 - accuracy: 0.7195 - val_loss: 0.8743 - val_accuracy: 0.7190\n",
            "Epoch 50/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8679 - accuracy: 0.7212 - val_loss: 0.8709 - val_accuracy: 0.7189\n",
            "Epoch 51/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8633 - accuracy: 0.7211 - val_loss: 0.8636 - val_accuracy: 0.7230\n",
            "Epoch 52/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8581 - accuracy: 0.7242 - val_loss: 0.8590 - val_accuracy: 0.7249\n",
            "Epoch 53/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8524 - accuracy: 0.7262 - val_loss: 0.8528 - val_accuracy: 0.7264\n",
            "Epoch 54/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8472 - accuracy: 0.7277 - val_loss: 0.8482 - val_accuracy: 0.7271\n",
            "Epoch 55/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 0.8419 - accuracy: 0.7294 - val_loss: 0.8449 - val_accuracy: 0.7321\n",
            "Epoch 56/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8377 - accuracy: 0.7301 - val_loss: 0.8399 - val_accuracy: 0.7311\n",
            "Epoch 57/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8330 - accuracy: 0.7329 - val_loss: 0.8376 - val_accuracy: 0.7327\n",
            "Epoch 58/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8281 - accuracy: 0.7342 - val_loss: 0.8331 - val_accuracy: 0.7287\n",
            "Epoch 59/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8242 - accuracy: 0.7350 - val_loss: 0.8273 - val_accuracy: 0.7329\n",
            "Epoch 60/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8191 - accuracy: 0.7367 - val_loss: 0.8212 - val_accuracy: 0.7337\n",
            "Epoch 61/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8159 - accuracy: 0.7388 - val_loss: 0.8187 - val_accuracy: 0.7355\n",
            "Epoch 62/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8116 - accuracy: 0.7386 - val_loss: 0.8136 - val_accuracy: 0.7387\n",
            "Epoch 63/9999\n",
            "48000/48000 [==============================] - 4s 79us/step - loss: 0.8074 - accuracy: 0.7410 - val_loss: 0.8094 - val_accuracy: 0.7358\n",
            "Epoch 64/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8035 - accuracy: 0.7421 - val_loss: 0.8063 - val_accuracy: 0.7408\n",
            "Epoch 65/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 0.7991 - accuracy: 0.7420 - val_loss: 0.8022 - val_accuracy: 0.7441\n",
            "Epoch 66/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7955 - accuracy: 0.7443 - val_loss: 0.7999 - val_accuracy: 0.7439\n",
            "Epoch 67/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7910 - accuracy: 0.7460 - val_loss: 0.7984 - val_accuracy: 0.7463\n",
            "Epoch 68/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7882 - accuracy: 0.7447 - val_loss: 0.7913 - val_accuracy: 0.7450\n",
            "Epoch 69/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7849 - accuracy: 0.7468 - val_loss: 0.7878 - val_accuracy: 0.7441\n",
            "Epoch 70/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7813 - accuracy: 0.7489 - val_loss: 0.7876 - val_accuracy: 0.7502\n",
            "Epoch 71/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7784 - accuracy: 0.7492 - val_loss: 0.7844 - val_accuracy: 0.7491\n",
            "10000/10000 [==============================] - 1s 91us/step\n",
            "Model: 5 added. Resulting score: 0.7574999928474426\n",
            "Train model 6\n",
            "Weight init method: Glorot Normal \n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/9999\n",
            "48000/48000 [==============================] - 4s 82us/step - loss: 2.2725 - accuracy: 0.1653 - val_loss: 2.2567 - val_accuracy: 0.1978\n",
            "Epoch 2/9999\n",
            "48000/48000 [==============================] - 4s 74us/step - loss: 2.1356 - accuracy: 0.3052 - val_loss: 2.0875 - val_accuracy: 0.3346\n",
            "Epoch 3/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 2.0135 - accuracy: 0.3628 - val_loss: 1.9435 - val_accuracy: 0.3723\n",
            "Epoch 4/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.8855 - accuracy: 0.3872 - val_loss: 1.8176 - val_accuracy: 0.4072\n",
            "Epoch 5/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.7722 - accuracy: 0.4162 - val_loss: 1.7172 - val_accuracy: 0.4277\n",
            "Epoch 6/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.6822 - accuracy: 0.4416 - val_loss: 1.6376 - val_accuracy: 0.4543\n",
            "Epoch 7/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.6088 - accuracy: 0.4637 - val_loss: 1.5714 - val_accuracy: 0.4797\n",
            "Epoch 8/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.5495 - accuracy: 0.4825 - val_loss: 1.5165 - val_accuracy: 0.4962\n",
            "Epoch 9/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.4989 - accuracy: 0.4985 - val_loss: 1.4698 - val_accuracy: 0.5071\n",
            "Epoch 10/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.4560 - accuracy: 0.5109 - val_loss: 1.4323 - val_accuracy: 0.5186\n",
            "Epoch 11/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.4184 - accuracy: 0.5210 - val_loss: 1.3949 - val_accuracy: 0.5306\n",
            "Epoch 12/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3853 - accuracy: 0.5312 - val_loss: 1.3643 - val_accuracy: 0.5348\n",
            "Epoch 13/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3556 - accuracy: 0.5415 - val_loss: 1.3358 - val_accuracy: 0.5414\n",
            "Epoch 14/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3273 - accuracy: 0.5498 - val_loss: 1.3126 - val_accuracy: 0.5513\n",
            "Epoch 15/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3043 - accuracy: 0.5571 - val_loss: 1.2895 - val_accuracy: 0.5564\n",
            "Epoch 16/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2826 - accuracy: 0.5650 - val_loss: 1.2657 - val_accuracy: 0.5681\n",
            "Epoch 17/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2606 - accuracy: 0.5724 - val_loss: 1.2464 - val_accuracy: 0.5754\n",
            "Epoch 18/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2410 - accuracy: 0.5796 - val_loss: 1.2294 - val_accuracy: 0.5798\n",
            "Epoch 19/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.2230 - accuracy: 0.5859 - val_loss: 1.2120 - val_accuracy: 0.5897\n",
            "Epoch 20/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.2056 - accuracy: 0.5935 - val_loss: 1.1948 - val_accuracy: 0.5943\n",
            "Epoch 21/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1895 - accuracy: 0.5990 - val_loss: 1.1782 - val_accuracy: 0.6023\n",
            "Epoch 22/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1725 - accuracy: 0.6055 - val_loss: 1.1638 - val_accuracy: 0.6087\n",
            "Epoch 23/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1580 - accuracy: 0.6107 - val_loss: 1.1514 - val_accuracy: 0.6127\n",
            "Epoch 24/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1439 - accuracy: 0.6175 - val_loss: 1.1371 - val_accuracy: 0.6166\n",
            "Epoch 25/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1299 - accuracy: 0.6227 - val_loss: 1.1235 - val_accuracy: 0.6183\n",
            "Epoch 26/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1166 - accuracy: 0.6274 - val_loss: 1.1111 - val_accuracy: 0.6298\n",
            "Epoch 27/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.1043 - accuracy: 0.6325 - val_loss: 1.0984 - val_accuracy: 0.6361\n",
            "Epoch 28/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0912 - accuracy: 0.6383 - val_loss: 1.0864 - val_accuracy: 0.6393\n",
            "Epoch 29/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 1.0798 - accuracy: 0.6441 - val_loss: 1.0754 - val_accuracy: 0.6476\n",
            "Epoch 30/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 1.0676 - accuracy: 0.6491 - val_loss: 1.0633 - val_accuracy: 0.6478\n",
            "Epoch 31/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 1.0567 - accuracy: 0.6524 - val_loss: 1.0525 - val_accuracy: 0.6541\n",
            "Epoch 32/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0458 - accuracy: 0.6589 - val_loss: 1.0435 - val_accuracy: 0.6581\n",
            "Epoch 33/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0352 - accuracy: 0.6626 - val_loss: 1.0317 - val_accuracy: 0.6613\n",
            "Epoch 34/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0252 - accuracy: 0.6665 - val_loss: 1.0242 - val_accuracy: 0.6662\n",
            "Epoch 35/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0146 - accuracy: 0.6708 - val_loss: 1.0144 - val_accuracy: 0.6637\n",
            "Epoch 36/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.0047 - accuracy: 0.6740 - val_loss: 1.0037 - val_accuracy: 0.6725\n",
            "Epoch 37/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9958 - accuracy: 0.6786 - val_loss: 0.9949 - val_accuracy: 0.6802\n",
            "Epoch 38/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9868 - accuracy: 0.6807 - val_loss: 0.9851 - val_accuracy: 0.6826\n",
            "Epoch 39/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9779 - accuracy: 0.6846 - val_loss: 0.9765 - val_accuracy: 0.6846\n",
            "Epoch 40/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9692 - accuracy: 0.6896 - val_loss: 0.9726 - val_accuracy: 0.6782\n",
            "Epoch 41/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9615 - accuracy: 0.6907 - val_loss: 0.9612 - val_accuracy: 0.6886\n",
            "Epoch 42/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9534 - accuracy: 0.6933 - val_loss: 0.9572 - val_accuracy: 0.6861\n",
            "Epoch 43/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9462 - accuracy: 0.6963 - val_loss: 0.9466 - val_accuracy: 0.6960\n",
            "Epoch 44/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9380 - accuracy: 0.7003 - val_loss: 0.9398 - val_accuracy: 0.6975\n",
            "Epoch 45/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9312 - accuracy: 0.7031 - val_loss: 0.9321 - val_accuracy: 0.7005\n",
            "Epoch 46/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9234 - accuracy: 0.7051 - val_loss: 0.9243 - val_accuracy: 0.7058\n",
            "Epoch 47/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.9166 - accuracy: 0.7070 - val_loss: 0.9193 - val_accuracy: 0.7028\n",
            "Epoch 48/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9098 - accuracy: 0.7111 - val_loss: 0.9138 - val_accuracy: 0.7080\n",
            "Epoch 49/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9035 - accuracy: 0.7130 - val_loss: 0.9066 - val_accuracy: 0.7105\n",
            "Epoch 50/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8974 - accuracy: 0.7137 - val_loss: 0.9005 - val_accuracy: 0.7121\n",
            "Epoch 51/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8909 - accuracy: 0.7168 - val_loss: 0.8933 - val_accuracy: 0.7183\n",
            "Epoch 52/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8853 - accuracy: 0.7186 - val_loss: 0.8887 - val_accuracy: 0.7212\n",
            "Epoch 53/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8793 - accuracy: 0.7212 - val_loss: 0.8825 - val_accuracy: 0.7219\n",
            "Epoch 54/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8730 - accuracy: 0.7221 - val_loss: 0.8762 - val_accuracy: 0.7218\n",
            "Epoch 55/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8679 - accuracy: 0.7235 - val_loss: 0.8702 - val_accuracy: 0.7247\n",
            "Epoch 56/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8620 - accuracy: 0.7261 - val_loss: 0.8651 - val_accuracy: 0.7294\n",
            "Epoch 57/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8572 - accuracy: 0.7265 - val_loss: 0.8599 - val_accuracy: 0.7286\n",
            "Epoch 58/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8514 - accuracy: 0.7282 - val_loss: 0.8598 - val_accuracy: 0.7327\n",
            "Epoch 59/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8461 - accuracy: 0.7312 - val_loss: 0.8509 - val_accuracy: 0.7296\n",
            "Epoch 60/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8423 - accuracy: 0.7320 - val_loss: 0.8453 - val_accuracy: 0.7312\n",
            "Epoch 61/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8369 - accuracy: 0.7343 - val_loss: 0.8436 - val_accuracy: 0.7352\n",
            "Epoch 62/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8319 - accuracy: 0.7360 - val_loss: 0.8378 - val_accuracy: 0.7349\n",
            "Epoch 63/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8276 - accuracy: 0.7385 - val_loss: 0.8316 - val_accuracy: 0.7377\n",
            "Epoch 64/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.8233 - accuracy: 0.7382 - val_loss: 0.8271 - val_accuracy: 0.7381\n",
            "Epoch 65/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8193 - accuracy: 0.7408 - val_loss: 0.8225 - val_accuracy: 0.7398\n",
            "Epoch 66/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8144 - accuracy: 0.7406 - val_loss: 0.8201 - val_accuracy: 0.7371\n",
            "Epoch 67/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8111 - accuracy: 0.7425 - val_loss: 0.8142 - val_accuracy: 0.7425\n",
            "Epoch 68/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8058 - accuracy: 0.7444 - val_loss: 0.8124 - val_accuracy: 0.7438\n",
            "Epoch 69/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8015 - accuracy: 0.7444 - val_loss: 0.8065 - val_accuracy: 0.7444\n",
            "Epoch 70/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7982 - accuracy: 0.7466 - val_loss: 0.8026 - val_accuracy: 0.7467\n",
            "Epoch 71/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7935 - accuracy: 0.7477 - val_loss: 0.8004 - val_accuracy: 0.7513\n",
            "Epoch 72/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7896 - accuracy: 0.7484 - val_loss: 0.7956 - val_accuracy: 0.7510\n",
            "Epoch 73/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7869 - accuracy: 0.7508 - val_loss: 0.7918 - val_accuracy: 0.7518\n",
            "Epoch 74/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7837 - accuracy: 0.7505 - val_loss: 0.7890 - val_accuracy: 0.7530\n",
            "Epoch 75/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 0.7797 - accuracy: 0.7525 - val_loss: 0.7864 - val_accuracy: 0.7514\n",
            "10000/10000 [==============================] - 1s 96us/step\n",
            "Model: 6 added. Resulting score: 0.7620000243186951\n",
            "Train model 7\n",
            "Weight init method: Glorot Uniform \n",
            "Train on 48000 samples, validate on 12000 samples\n",
            "Epoch 1/9999\n",
            "48000/48000 [==============================] - 4s 82us/step - loss: 2.2526 - accuracy: 0.1454 - val_loss: 2.2618 - val_accuracy: 0.2245\n",
            "Epoch 2/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 2.1396 - accuracy: 0.2795 - val_loss: 2.0894 - val_accuracy: 0.3349\n",
            "Epoch 3/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 2.0126 - accuracy: 0.3634 - val_loss: 1.9419 - val_accuracy: 0.3739\n",
            "Epoch 4/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.8855 - accuracy: 0.3935 - val_loss: 1.8208 - val_accuracy: 0.4087\n",
            "Epoch 5/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.7728 - accuracy: 0.4312 - val_loss: 1.7203 - val_accuracy: 0.4460\n",
            "Epoch 6/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.6832 - accuracy: 0.4531 - val_loss: 1.6421 - val_accuracy: 0.4643\n",
            "Epoch 7/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.6121 - accuracy: 0.4734 - val_loss: 1.5798 - val_accuracy: 0.4790\n",
            "Epoch 8/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.5535 - accuracy: 0.4915 - val_loss: 1.5256 - val_accuracy: 0.4935\n",
            "Epoch 9/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 1.5023 - accuracy: 0.5072 - val_loss: 1.4790 - val_accuracy: 0.5060\n",
            "Epoch 10/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.4582 - accuracy: 0.5216 - val_loss: 1.4387 - val_accuracy: 0.5191\n",
            "Epoch 11/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.4197 - accuracy: 0.5328 - val_loss: 1.4010 - val_accuracy: 0.5321\n",
            "Epoch 12/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.3853 - accuracy: 0.5427 - val_loss: 1.3682 - val_accuracy: 0.5424\n",
            "Epoch 13/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3538 - accuracy: 0.5525 - val_loss: 1.3384 - val_accuracy: 0.5565\n",
            "Epoch 14/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.3246 - accuracy: 0.5626 - val_loss: 1.3113 - val_accuracy: 0.5685\n",
            "Epoch 15/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2994 - accuracy: 0.5698 - val_loss: 1.2883 - val_accuracy: 0.5746\n",
            "Epoch 16/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.2756 - accuracy: 0.5767 - val_loss: 1.2639 - val_accuracy: 0.5850\n",
            "Epoch 17/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.2544 - accuracy: 0.5849 - val_loss: 1.2445 - val_accuracy: 0.5869\n",
            "Epoch 18/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.2341 - accuracy: 0.5924 - val_loss: 1.2227 - val_accuracy: 0.6025\n",
            "Epoch 19/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.2139 - accuracy: 0.6002 - val_loss: 1.2059 - val_accuracy: 0.6087\n",
            "Epoch 20/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.1959 - accuracy: 0.6083 - val_loss: 1.1873 - val_accuracy: 0.6125\n",
            "Epoch 21/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1788 - accuracy: 0.6141 - val_loss: 1.1722 - val_accuracy: 0.6181\n",
            "Epoch 22/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1631 - accuracy: 0.6215 - val_loss: 1.1570 - val_accuracy: 0.6242\n",
            "Epoch 23/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1474 - accuracy: 0.6274 - val_loss: 1.1406 - val_accuracy: 0.6338\n",
            "Epoch 24/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1338 - accuracy: 0.6332 - val_loss: 1.1276 - val_accuracy: 0.6369\n",
            "Epoch 25/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1196 - accuracy: 0.6395 - val_loss: 1.1127 - val_accuracy: 0.6412\n",
            "Epoch 26/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.1065 - accuracy: 0.6434 - val_loss: 1.1037 - val_accuracy: 0.6470\n",
            "Epoch 27/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0938 - accuracy: 0.6496 - val_loss: 1.0895 - val_accuracy: 0.6474\n",
            "Epoch 28/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0822 - accuracy: 0.6547 - val_loss: 1.0771 - val_accuracy: 0.6582\n",
            "Epoch 29/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 1.0703 - accuracy: 0.6591 - val_loss: 1.0651 - val_accuracy: 0.6628\n",
            "Epoch 30/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0585 - accuracy: 0.6645 - val_loss: 1.0542 - val_accuracy: 0.6668\n",
            "Epoch 31/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0469 - accuracy: 0.6686 - val_loss: 1.0434 - val_accuracy: 0.6640\n",
            "Epoch 32/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0368 - accuracy: 0.6715 - val_loss: 1.0329 - val_accuracy: 0.6727\n",
            "Epoch 33/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0273 - accuracy: 0.6755 - val_loss: 1.0225 - val_accuracy: 0.6802\n",
            "Epoch 34/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0160 - accuracy: 0.6784 - val_loss: 1.0156 - val_accuracy: 0.6839\n",
            "Epoch 35/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 1.0062 - accuracy: 0.6833 - val_loss: 1.0036 - val_accuracy: 0.6799\n",
            "Epoch 36/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9970 - accuracy: 0.6857 - val_loss: 0.9938 - val_accuracy: 0.6886\n",
            "Epoch 37/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9880 - accuracy: 0.6891 - val_loss: 0.9864 - val_accuracy: 0.6833\n",
            "Epoch 38/9999\n",
            "48000/48000 [==============================] - 4s 78us/step - loss: 0.9783 - accuracy: 0.6925 - val_loss: 0.9764 - val_accuracy: 0.6926\n",
            "Epoch 39/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 0.9700 - accuracy: 0.6956 - val_loss: 0.9687 - val_accuracy: 0.6924\n",
            "Epoch 40/9999\n",
            "48000/48000 [==============================] - 4s 80us/step - loss: 0.9614 - accuracy: 0.6980 - val_loss: 0.9623 - val_accuracy: 0.6941\n",
            "Epoch 41/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9530 - accuracy: 0.7015 - val_loss: 0.9503 - val_accuracy: 0.7018\n",
            "Epoch 42/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9457 - accuracy: 0.7029 - val_loss: 0.9437 - val_accuracy: 0.7023\n",
            "Epoch 43/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9372 - accuracy: 0.7075 - val_loss: 0.9361 - val_accuracy: 0.7078\n",
            "Epoch 44/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9297 - accuracy: 0.7092 - val_loss: 0.9307 - val_accuracy: 0.7041\n",
            "Epoch 45/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9224 - accuracy: 0.7106 - val_loss: 0.9210 - val_accuracy: 0.7132\n",
            "Epoch 46/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9145 - accuracy: 0.7148 - val_loss: 0.9158 - val_accuracy: 0.7138\n",
            "Epoch 47/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.9088 - accuracy: 0.7151 - val_loss: 0.9081 - val_accuracy: 0.7128\n",
            "Epoch 48/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.9018 - accuracy: 0.7177 - val_loss: 0.9037 - val_accuracy: 0.7167\n",
            "Epoch 49/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8954 - accuracy: 0.7203 - val_loss: 0.8955 - val_accuracy: 0.7218\n",
            "Epoch 50/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8885 - accuracy: 0.7222 - val_loss: 0.8918 - val_accuracy: 0.7192\n",
            "Epoch 51/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8816 - accuracy: 0.7236 - val_loss: 0.8822 - val_accuracy: 0.7240\n",
            "Epoch 52/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8762 - accuracy: 0.7265 - val_loss: 0.8771 - val_accuracy: 0.7260\n",
            "Epoch 53/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8705 - accuracy: 0.7276 - val_loss: 0.8717 - val_accuracy: 0.7221\n",
            "Epoch 54/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8635 - accuracy: 0.7302 - val_loss: 0.8642 - val_accuracy: 0.7284\n",
            "Epoch 55/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8579 - accuracy: 0.7317 - val_loss: 0.8593 - val_accuracy: 0.7317\n",
            "Epoch 56/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8516 - accuracy: 0.7328 - val_loss: 0.8551 - val_accuracy: 0.7298\n",
            "Epoch 57/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8471 - accuracy: 0.7350 - val_loss: 0.8501 - val_accuracy: 0.7289\n",
            "Epoch 58/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8423 - accuracy: 0.7380 - val_loss: 0.8453 - val_accuracy: 0.7311\n",
            "Epoch 59/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8376 - accuracy: 0.7371 - val_loss: 0.8395 - val_accuracy: 0.7377\n",
            "Epoch 60/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8324 - accuracy: 0.7412 - val_loss: 0.8334 - val_accuracy: 0.7380\n",
            "Epoch 61/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8274 - accuracy: 0.7412 - val_loss: 0.8306 - val_accuracy: 0.7362\n",
            "Epoch 62/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8221 - accuracy: 0.7433 - val_loss: 0.8254 - val_accuracy: 0.7394\n",
            "Epoch 63/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8167 - accuracy: 0.7446 - val_loss: 0.8190 - val_accuracy: 0.7427\n",
            "Epoch 64/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.8111 - accuracy: 0.7474 - val_loss: 0.8143 - val_accuracy: 0.7458\n",
            "Epoch 65/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8063 - accuracy: 0.7480 - val_loss: 0.8100 - val_accuracy: 0.7471\n",
            "Epoch 66/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.8031 - accuracy: 0.7495 - val_loss: 0.8055 - val_accuracy: 0.7488\n",
            "Epoch 67/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7981 - accuracy: 0.7514 - val_loss: 0.8005 - val_accuracy: 0.7502\n",
            "Epoch 68/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7938 - accuracy: 0.7532 - val_loss: 0.7962 - val_accuracy: 0.7516\n",
            "Epoch 69/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7889 - accuracy: 0.7541 - val_loss: 0.7936 - val_accuracy: 0.7499\n",
            "Epoch 70/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7839 - accuracy: 0.7551 - val_loss: 0.7881 - val_accuracy: 0.7540\n",
            "Epoch 71/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7808 - accuracy: 0.7560 - val_loss: 0.7866 - val_accuracy: 0.7543\n",
            "Epoch 72/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7756 - accuracy: 0.7584 - val_loss: 0.7803 - val_accuracy: 0.7585\n",
            "Epoch 73/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7715 - accuracy: 0.7602 - val_loss: 0.7764 - val_accuracy: 0.7593\n",
            "Epoch 74/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7669 - accuracy: 0.7610 - val_loss: 0.7763 - val_accuracy: 0.7561\n",
            "Epoch 75/9999\n",
            "48000/48000 [==============================] - 4s 77us/step - loss: 0.7635 - accuracy: 0.7620 - val_loss: 0.7683 - val_accuracy: 0.7595\n",
            "Epoch 76/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7593 - accuracy: 0.7632 - val_loss: 0.7653 - val_accuracy: 0.7603\n",
            "Epoch 77/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7553 - accuracy: 0.7637 - val_loss: 0.7623 - val_accuracy: 0.7604\n",
            "Epoch 78/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7515 - accuracy: 0.7658 - val_loss: 0.7570 - val_accuracy: 0.7627\n",
            "Epoch 79/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7473 - accuracy: 0.7661 - val_loss: 0.7546 - val_accuracy: 0.7632\n",
            "Epoch 80/9999\n",
            "48000/48000 [==============================] - 4s 76us/step - loss: 0.7445 - accuracy: 0.7670 - val_loss: 0.7527 - val_accuracy: 0.7659\n",
            "Epoch 81/9999\n",
            "48000/48000 [==============================] - 4s 75us/step - loss: 0.7397 - accuracy: 0.7688 - val_loss: 0.7484 - val_accuracy: 0.7666\n",
            "10000/10000 [==============================] - 1s 92us/step\n",
            "Model: 7 added. Resulting score: 0.7757999897003174\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fGT6jV-hcLbJ"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CurcmjMCcrJI"
      },
      "source": [
        "# Accuracy vs Weight initialization method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yvZLQyb5cg7R",
        "outputId": "5bcc1049-7d96-4214-ec31-ec1020149892",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "accuracy_df = pd.DataFrame(accuracies, columns=[\"Accuracy\"])\n",
        "accuracy_df[\"weight_init_method\"] = initializer\n",
        "display(accuracy_df)\n",
        "\n",
        "accuracy_df.to_csv(PATH + MODEL_NAME + \"_accuracy_\"+ run + \".csv\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>weight_init_method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.7627</td>\n",
              "      <td>Zero</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.7724</td>\n",
              "      <td>Ones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.7837</td>\n",
              "      <td>Random Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.7621</td>\n",
              "      <td>Random Uniform</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.7611</td>\n",
              "      <td>Identity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.7575</td>\n",
              "      <td>Orthogonal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.7620</td>\n",
              "      <td>Glorot Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.7758</td>\n",
              "      <td>Glorot Uniform</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Accuracy weight_init_method\n",
              "0    0.7627               Zero\n",
              "1    0.7724               Ones\n",
              "2    0.7837      Random Normal\n",
              "3    0.7621     Random Uniform\n",
              "4    0.7611           Identity\n",
              "5    0.7575         Orthogonal\n",
              "6    0.7620      Glorot Normal\n",
              "7    0.7758     Glorot Uniform"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sXq8Uj3lenzH",
        "outputId": "94f49bd7-7a45-4e46-ea50-eee9e3ec84fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 358
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "accuracy_df.plot(x=\"weight_init_method\", y=\"Accuracy\",rot = 90)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O37chbRgZDXF",
        "colab_type": "code",
        "outputId": "e89d2156-1893-40c4-e3e4-3692e5b732a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "classified = []\n",
        "\n",
        "for prediction in tqdm(predictions):\n",
        "    classified.append([1 if i==j else 0 for i,j in zip(prediction,y_test)])"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:00<00:00, 190.13it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "NMfc-h8xAYQu"
      },
      "source": [
        "## Correlation between models"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FAky42lMV102",
        "outputId": "1104852f-9bff-4356-dec0-274a4fa178e5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 314
        }
      },
      "source": [
        "correlation_matrix = []\n",
        "\n",
        "for ix, x in enumerate(classified):\n",
        "  row = []\n",
        "  \n",
        "  for iy, y in enumerate(classified):\n",
        "    if (ix == iy):\n",
        "      row.append(np.nan)\n",
        "    else:\n",
        "      row.append(pearsonr(x,y)[0])\n",
        "\n",
        "  correlation_matrix.append(row)\n",
        "\n",
        "correlation_matrix = np.array(correlation_matrix)\n",
        "correlation_matrix_df = pd.DataFrame(correlation_matrix)\n",
        "correlation_matrix_df.columns = initializer\n",
        "correlation_matrix_df.index = initializer\n",
        "display(correlation_matrix_df)\n",
        "print(\"Average correlation: \" + str(np.nanmean(correlation_matrix.flatten())))\n",
        "correlation_matrix_df.to_csv(PATH + MODEL_NAME + \"_correlation_matrix_\" + run + \".csv\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Zero</th>\n",
              "      <th>Ones</th>\n",
              "      <th>Random Normal</th>\n",
              "      <th>Random Uniform</th>\n",
              "      <th>Identity</th>\n",
              "      <th>Orthogonal</th>\n",
              "      <th>Glorot Normal</th>\n",
              "      <th>Glorot Uniform</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Zero</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.648582</td>\n",
              "      <td>0.639264</td>\n",
              "      <td>0.621301</td>\n",
              "      <td>0.652174</td>\n",
              "      <td>0.704494</td>\n",
              "      <td>0.702771</td>\n",
              "      <td>0.676321</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ones</th>\n",
              "      <td>0.648582</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.626609</td>\n",
              "      <td>0.649494</td>\n",
              "      <td>0.657354</td>\n",
              "      <td>0.651115</td>\n",
              "      <td>0.639752</td>\n",
              "      <td>0.653492</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Normal</th>\n",
              "      <td>0.639264</td>\n",
              "      <td>0.626609</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.623708</td>\n",
              "      <td>0.618161</td>\n",
              "      <td>0.650834</td>\n",
              "      <td>0.633191</td>\n",
              "      <td>0.664524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Uniform</th>\n",
              "      <td>0.621301</td>\n",
              "      <td>0.649494</td>\n",
              "      <td>0.623708</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.628236</td>\n",
              "      <td>0.644453</td>\n",
              "      <td>0.632438</td>\n",
              "      <td>0.644005</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Identity</th>\n",
              "      <td>0.652174</td>\n",
              "      <td>0.657354</td>\n",
              "      <td>0.618161</td>\n",
              "      <td>0.628236</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.648217</td>\n",
              "      <td>0.646186</td>\n",
              "      <td>0.640692</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Orthogonal</th>\n",
              "      <td>0.704494</td>\n",
              "      <td>0.651115</td>\n",
              "      <td>0.650834</td>\n",
              "      <td>0.644453</td>\n",
              "      <td>0.648217</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.696823</td>\n",
              "      <td>0.653053</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Glorot Normal</th>\n",
              "      <td>0.702771</td>\n",
              "      <td>0.639752</td>\n",
              "      <td>0.633191</td>\n",
              "      <td>0.632438</td>\n",
              "      <td>0.646186</td>\n",
              "      <td>0.696823</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.657861</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Glorot Uniform</th>\n",
              "      <td>0.676321</td>\n",
              "      <td>0.653492</td>\n",
              "      <td>0.664524</td>\n",
              "      <td>0.644005</td>\n",
              "      <td>0.640692</td>\n",
              "      <td>0.653053</td>\n",
              "      <td>0.657861</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Zero      Ones  ...  Glorot Normal  Glorot Uniform\n",
              "Zero                 NaN  0.648582  ...       0.702771        0.676321\n",
              "Ones            0.648582       NaN  ...       0.639752        0.653492\n",
              "Random Normal   0.639264  0.626609  ...       0.633191        0.664524\n",
              "Random Uniform  0.621301  0.649494  ...       0.632438        0.644005\n",
              "Identity        0.652174  0.657354  ...       0.646186        0.640692\n",
              "Orthogonal      0.704494  0.651115  ...       0.696823        0.653053\n",
              "Glorot Normal   0.702771  0.639752  ...            NaN        0.657861\n",
              "Glorot Uniform  0.676321  0.653492  ...       0.657861             NaN\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Average correlation: 0.6501823095907168\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dvzckea_u1ZU",
        "colab_type": "text"
      },
      "source": [
        "# Ensemble accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMA1TEn7u2TC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def predict(models, X, Y):\n",
        "    predictions = []\n",
        "\n",
        "    for m in tqdm(models):\n",
        "        predictions.append(np.argmax(m.predict(X), axis=1))\n",
        "\n",
        "    prediction = np.transpose(predictions)\n",
        "    prediction = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=prediction)\n",
        "\n",
        "    return accuracy_score(prediction, np.argmax(Y, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTcKy5Pmu2o_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "af305287-43ed-4fb0-8eee-a3ad6fa4869d"
      },
      "source": [
        "print(\"Accuracy of ensemble: \" + str(predict(models, x_test, y_testc)))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 8/8 [00:04<00:00,  1.90it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Accuracy of ensemble: 0.7926\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}