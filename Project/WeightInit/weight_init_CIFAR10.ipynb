{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of weight_int_CIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fde69AMuOpox",
        "outputId": "b481ec1a-531d-4364-e319-f1474270162c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import initializers\n",
        "from itertools import count\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import cifar10\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras import layers\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, Input, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
        "from keras.models import Model\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping, CSVLogger\n",
        "from scipy.stats import pearsonr\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qYrab7qpOppj",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 9999\n",
        "IMAGE_SIZE = 32\n",
        "NUM_CLASSES = 10\n",
        "NUM_CHANNELS = 3\n",
        "MODEL_NAME = \"CIFAR10_weight_init\"\n",
        "PATH = \"\"\n",
        "NR_OF_RUNS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "g8QvEt97vF52"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "JtJIUBsFKeRO",
        "outputId": "fecd1037-673a-4b23-9206-5cd00b2ec07b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XypdmBJROpp9",
        "colab": {}
      },
      "source": [
        "# Convert class vectors to binary class matrices.\n",
        "y_trainc = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_testc = keras.utils.to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mo8yHyg-Opqo",
        "colab": {}
      },
      "source": [
        "x_train_full = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train_full /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "gIBGIrlkvOt0"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zLWph6_aOpr2",
        "colab": {}
      },
      "source": [
        "def CIFAR10model(imsize, num_classes, num_channels):\n",
        "    inputs = Input((imsize,imsize,num_channels))\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(inputs)\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3), strides=2)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(x)\n",
        "    x = Conv2D(filters=128, kernel_size=(3,3), activation='relu', strides=2, padding='same')(x)\n",
        "    x = Conv2D(filters=128, kernel_size=(3,3), activation='relu', strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Conv2D(filters=128, kernel_size=(1,1), activation='relu', padding='valid')(x)\n",
        "    x = Conv2D(filters=10, kernel_size=(1,1),strides=(1,1), padding='valid')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    outputs = Activation('softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "    optimizer = keras.optimizers.Adam(learning_rate = 1e-04)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=['accuracy'])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zP2b1rgCbTru"
      },
      "source": [
        "# Predict"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FvCw4pprbTru",
        "colab": {}
      },
      "source": [
        "def hard_voting(models, X):\n",
        "    predictions = []\n",
        "\n",
        "    for m in models:\n",
        "        predictions.append(np.argmax(m.predict(X), axis=1))\n",
        "\n",
        "    prediction = np.transpose(predictions)\n",
        "    prediction = np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=prediction)\n",
        "\n",
        "    return prediction\n",
        "\n",
        "def soft_voting(models, X):\n",
        "    predictions = np.empty((len(X),0,NUM_CLASSES))\n",
        "\n",
        "    for m in models:\n",
        "        pred = np.expand_dims(m.predict(X), axis=1)\n",
        "        predictions = np.append(predictions, pred, axis=1)\n",
        "\n",
        "    predictions = np.apply_along_axis(np.transpose, axis=1, arr=predictions)\n",
        "    predictions = np.mean(predictions, axis=1)\n",
        "    prediction = np.argmax(predictions, axis=1)\n",
        "\n",
        "    return prediction\n",
        "\n",
        "def predict(models, X, Y,voting = 'hard'):\n",
        "    \n",
        "    if voting == \"soft\":\n",
        "      prediction = soft_voting(models, X)\n",
        "    elif voting == \"hard\":\n",
        "      prediction = hard_voting(models, X)\n",
        "    else:\n",
        "      raise ValueError(f\"Voting mechanism: {VOTING} not supported\")\n",
        "\n",
        "    return accuracy_score(prediction, np.argmax(Y, axis=1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TVqdcrD_vQ-Q"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HjvZqLBJOpsw",
        "outputId": "4b94f3c2-0726-4a3f-d209-79aa56a4a4d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "for run in range(1, NR_OF_RUNS+1):\n",
        "    \n",
        "    # Split the data\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_trainc, test_size=0.20, shuffle= True)\n",
        "    \n",
        "    models = []\n",
        "    accuracies = []\n",
        "    predictions = []\n",
        "    initializer = [\"Zero\",\"Ones\",\"Random Normal\",\"Random Uniform\",\"Identity\",\"Orthogonal\",\"Glorot Normal\",\"Glorot Uniform\"]\n",
        "    for i in range(len(initializer)):\n",
        "        print(f\"\\n ===== Train model: Weight init method: {initializer[i]}  =====\")\n",
        "        \n",
        "        # Set the seeds\n",
        "        np.random.seed(run*i)\n",
        "        tf.random.set_seed(run*i)\n",
        "\n",
        "        # Create directories\n",
        "        os.makedirs(PATH + MODEL_NAME + f\"/{run}/history\", exist_ok=True)\n",
        "        os.makedirs(PATH + MODEL_NAME + f\"/{run}/weights\", exist_ok=True)\n",
        "        \n",
        "        # weight init method\n",
        "        model = CIFAR10model(IMAGE_SIZE,NUM_CLASSES,NUM_CHANNELS)\n",
        "    \n",
        "        for layer in model.layers: \n",
        "            if hasattr(layer, 'kernel_initializer'):\n",
        "                if(initializer[i] == \"Zero\"):\n",
        "                    layer.kernel_initializer = initializers.Zeros()\n",
        "                elif(initializer[i] == \"Ones\"):\n",
        "                    layer.kernel_initializer = initializers.Ones()\n",
        "                elif(initializer[i] == \"Random Normal\"):\n",
        "                    layer.kernel_initializer = initializers.RandomNormal()\n",
        "                elif(initializer[i] == \"Random Unifrom\"):\n",
        "                    layer.kernel_initializer = initializers.RandomUniform()\n",
        "                elif(initializer[i] == \"Identity\"):\n",
        "                    layer.kernel_initializer = initializers.Identity()\n",
        "                elif(initializer[i] == \"Orthogonal\"):\n",
        "                    layer.kernel_initializer = initializers.Orthogonal()\n",
        "                elif(initializer[i] == \"Glorot Normal\"):\n",
        "                    layer.kernel_initializer = initializers.GlorotNormal()\n",
        "                elif(initializer[i] == \"Glorot Unifrom\"):\n",
        "                    layer.kernel_initializer = initializers.GlorotUnifrom()\n",
        "            \n",
        "        #save weights \n",
        "        weights_path = PATH + MODEL_NAME + f\"/{run}/weights/weights-{initializer[i]}.h5\"\n",
        "        if os.path.exists(weights_path):\n",
        "            print(f\"Skipping training of model {initializer[i]}: weights exists\")\n",
        "            model.load_weights(weights_path)\n",
        "        else:\n",
        "            # initiate early stopping\n",
        "            es = EarlyStopping(min_delta=0.01, patience=3)\n",
        "            csv_logger = CSVLogger(PATH + MODEL_NAME + f\"/{run}/history/history-{initializer[i]}.csv\", separator=';')\n",
        "            #train\n",
        "            model.fit(x_train,y_train,\n",
        "                      batch_size = BATCH_SIZE,\n",
        "                      epochs = EPOCHS,\n",
        "                      validation_data = (x_val,y_val),\n",
        "                      shuffle = True,\n",
        "                      callbacks=[es, csv_logger])\n",
        "            model.save_weights(weights_path)\n",
        "            \n",
        "        models.append(model)\n",
        "        y_prob = model.predict(x_test) \n",
        "        predictions.append(y_prob.argmax(axis=-1))\n",
        "        acc = model.evaluate(x_test,y_testc)[1]\n",
        "        accuracies.append(acc)\n",
        "\n",
        "        print(f\"Model: {initializer[i]} added. Resulting score: {acc}\")\n",
        "        \n",
        "    # Results  \n",
        "        \n",
        "    # Accuracy vs Weight initialization method\n",
        "        \n",
        "    print(\"\\n ===== Accuracy vs weight init methods =====\")\n",
        "    accuracy_df = pd.DataFrame(accuracies, columns=[\"Accuracy\"])\n",
        "    accuracy_df[\"weight_init_method\"] = initializer\n",
        "    display(accuracy_df)\n",
        "    accuracy_df.to_csv(PATH + MODEL_NAME + f\"/{run}/accuracy.csv\")\n",
        "        \n",
        "    print(\"\\n ===== Converting Binary classification =====\")\n",
        "    classified = []\n",
        "    for prediction in tqdm(predictions):\n",
        "        classified.append([1 if i==j else 0 for i,j in zip(prediction,y_test)])\n",
        "        \n",
        "    ## Correlation between models\n",
        "    print(\"\\n ===== Correlation =====\")  \n",
        "    correlation_matrix = []\n",
        "\n",
        "    for ix, x in enumerate(classified):\n",
        "        row = []\n",
        "  \n",
        "        for iy, y in enumerate(classified):\n",
        "            if (ix == iy):\n",
        "                row.append(np.nan)\n",
        "            else:\n",
        "                row.append(pearsonr(x,y)[0])\n",
        "\n",
        "        correlation_matrix.append(row)\n",
        "\n",
        "    correlation_matrix = np.array(correlation_matrix)\n",
        "    correlation_matrix_df = pd.DataFrame(correlation_matrix)\n",
        "    correlation_matrix_df.columns = initializer\n",
        "    correlation_matrix_df.index = initializer\n",
        "    correlation_matrix_df.to_csv(PATH + MODEL_NAME + f\"/{run}/correlation_matrix.csv\")\n",
        "    display(correlation_matrix_df)\n",
        "    correlation = np.nanmean(correlation_matrix.flatten())\n",
        "    print(\"Average correlation: \" + str(correlation))\n",
        "    \n",
        "    \n",
        "    print(\"\\n ===== Computing ensemble accuracy =====\")  \n",
        "    # Ensemble accuracy\n",
        "    accuracy_hard = predict(models, x_test, y_testc,voting = 'hard')\n",
        "    print(\"Accuracy of ensemble using hard voting: \" + str(accuracy_hard))\n",
        "    accuracy_soft = predict(models, x_test, y_testc,voting = 'soft')\n",
        "    print(\"Accuracy of ensemble using soft voting: \" + str(accuracy_soft))\n",
        "    \n",
        "    \n",
        "    print(\"\\n ===== Computing ensemble accuracy =====\")  \n",
        "    # Save the results\n",
        "    file = PATH + MODEL_NAME + f\"/results_.csv\"\n",
        "    df = pd.DataFrame([[run,correlation,accuracy_hard,accuracy_soft]])\n",
        "\n",
        "    if not os.path.isfile(file):\n",
        "        df.to_csv(file, header=[\"run\", \"correlation\",\"accuracy_hard_voting\",\"accuracy_soft_voting\"], index=False)\n",
        "    else: # else it exists so append without writing the header\n",
        "        df.to_csv(file, mode='a', header=False, index=False)\n",
        "\n",
        "    clear_output(wait=True)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ===== Train model: Weight init method: Zero  =====\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/9999\n",
            "40000/40000 [==============================] - 5s 130us/step - loss: 1.7185 - accuracy: 0.4000 - val_loss: 2.4328 - val_accuracy: 0.1429\n",
            "Epoch 2/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 1.3177 - accuracy: 0.5391 - val_loss: 1.4761 - val_accuracy: 0.4855\n",
            "Epoch 3/9999\n",
            "40000/40000 [==============================] - 5s 118us/step - loss: 1.1529 - accuracy: 0.5972 - val_loss: 1.2068 - val_accuracy: 0.5734\n",
            "Epoch 4/9999\n",
            "40000/40000 [==============================] - 5s 117us/step - loss: 1.0451 - accuracy: 0.6367 - val_loss: 1.1447 - val_accuracy: 0.5894\n",
            "Epoch 5/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.9585 - accuracy: 0.6708 - val_loss: 1.0728 - val_accuracy: 0.6171\n",
            "Epoch 6/9999\n",
            "40000/40000 [==============================] - 5s 116us/step - loss: 0.8882 - accuracy: 0.6956 - val_loss: 1.1009 - val_accuracy: 0.6131\n",
            "Epoch 7/9999\n",
            "40000/40000 [==============================] - 5s 117us/step - loss: 0.8206 - accuracy: 0.7206 - val_loss: 0.9955 - val_accuracy: 0.6487\n",
            "Epoch 8/9999\n",
            "40000/40000 [==============================] - 5s 114us/step - loss: 0.7612 - accuracy: 0.7421 - val_loss: 1.0029 - val_accuracy: 0.6461\n",
            "Epoch 9/9999\n",
            "40000/40000 [==============================] - 5s 114us/step - loss: 0.7025 - accuracy: 0.7647 - val_loss: 1.0577 - val_accuracy: 0.6317\n",
            "Epoch 10/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 0.6501 - accuracy: 0.7830 - val_loss: 0.9935 - val_accuracy: 0.6534\n",
            "10000/10000 [==============================] - 1s 126us/step\n",
            "Model: Zero added. Resulting score: 0.6603999733924866\n",
            "\n",
            " ===== Train model: Weight init method: Ones  =====\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/9999\n",
            "40000/40000 [==============================] - 5s 130us/step - loss: 1.7199 - accuracy: 0.3887 - val_loss: 2.1434 - val_accuracy: 0.2116\n",
            "Epoch 2/9999\n",
            "40000/40000 [==============================] - 4s 112us/step - loss: 1.3317 - accuracy: 0.5285 - val_loss: 1.4316 - val_accuracy: 0.4937\n",
            "Epoch 3/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 1.1811 - accuracy: 0.5846 - val_loss: 1.2936 - val_accuracy: 0.5421\n",
            "Epoch 4/9999\n",
            "40000/40000 [==============================] - 5s 116us/step - loss: 1.0731 - accuracy: 0.6244 - val_loss: 1.1262 - val_accuracy: 0.6008\n",
            "Epoch 5/9999\n",
            "40000/40000 [==============================] - 5s 119us/step - loss: 0.9817 - accuracy: 0.6575 - val_loss: 1.1571 - val_accuracy: 0.5825\n",
            "Epoch 6/9999\n",
            "40000/40000 [==============================] - 5s 117us/step - loss: 0.9033 - accuracy: 0.6887 - val_loss: 1.0692 - val_accuracy: 0.6213\n",
            "Epoch 7/9999\n",
            "40000/40000 [==============================] - 5s 118us/step - loss: 0.8379 - accuracy: 0.7122 - val_loss: 1.0827 - val_accuracy: 0.6214\n",
            "Epoch 8/9999\n",
            "40000/40000 [==============================] - 5s 119us/step - loss: 0.7724 - accuracy: 0.7368 - val_loss: 1.0089 - val_accuracy: 0.6421\n",
            "Epoch 9/9999\n",
            "40000/40000 [==============================] - 5s 118us/step - loss: 0.7177 - accuracy: 0.7559 - val_loss: 1.2187 - val_accuracy: 0.5860\n",
            "Epoch 10/9999\n",
            "40000/40000 [==============================] - 5s 119us/step - loss: 0.6617 - accuracy: 0.7791 - val_loss: 0.9879 - val_accuracy: 0.6544\n",
            "Epoch 11/9999\n",
            "40000/40000 [==============================] - 5s 116us/step - loss: 0.6138 - accuracy: 0.7959 - val_loss: 1.0200 - val_accuracy: 0.6445\n",
            "Epoch 12/9999\n",
            "40000/40000 [==============================] - 5s 114us/step - loss: 0.5641 - accuracy: 0.8136 - val_loss: 1.0868 - val_accuracy: 0.6363\n",
            "Epoch 13/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 0.5169 - accuracy: 0.8305 - val_loss: 1.0069 - val_accuracy: 0.6578\n",
            "10000/10000 [==============================] - 1s 117us/step\n",
            "Model: Ones added. Resulting score: 0.6568999886512756\n",
            "\n",
            " ===== Train model: Weight init method: Random Normal  =====\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/9999\n",
            "40000/40000 [==============================] - 5s 135us/step - loss: 1.7256 - accuracy: 0.3893 - val_loss: 2.2076 - val_accuracy: 0.1550\n",
            "Epoch 2/9999\n",
            "40000/40000 [==============================] - 5s 116us/step - loss: 1.3444 - accuracy: 0.5257 - val_loss: 1.4960 - val_accuracy: 0.4798\n",
            "Epoch 3/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 1.1766 - accuracy: 0.5862 - val_loss: 1.2940 - val_accuracy: 0.5437\n",
            "Epoch 4/9999\n",
            "40000/40000 [==============================] - 5s 120us/step - loss: 1.0646 - accuracy: 0.6304 - val_loss: 1.2034 - val_accuracy: 0.5822\n",
            "Epoch 5/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 0.9745 - accuracy: 0.6639 - val_loss: 1.1269 - val_accuracy: 0.5984\n",
            "Epoch 6/9999\n",
            "40000/40000 [==============================] - 5s 119us/step - loss: 0.9038 - accuracy: 0.6890 - val_loss: 1.1350 - val_accuracy: 0.6048\n",
            "Epoch 7/9999\n",
            "40000/40000 [==============================] - 5s 120us/step - loss: 0.8392 - accuracy: 0.7141 - val_loss: 1.1310 - val_accuracy: 0.6046\n",
            "Epoch 8/9999\n",
            "40000/40000 [==============================] - 5s 124us/step - loss: 0.7817 - accuracy: 0.7354 - val_loss: 1.0023 - val_accuracy: 0.6476\n",
            "Epoch 9/9999\n",
            "40000/40000 [==============================] - 5s 122us/step - loss: 0.7247 - accuracy: 0.7572 - val_loss: 1.0108 - val_accuracy: 0.6468\n",
            "Epoch 10/9999\n",
            "40000/40000 [==============================] - 5s 119us/step - loss: 0.6703 - accuracy: 0.7759 - val_loss: 1.0466 - val_accuracy: 0.6353\n",
            "Epoch 11/9999\n",
            "40000/40000 [==============================] - 5s 120us/step - loss: 0.6230 - accuracy: 0.7943 - val_loss: 1.0306 - val_accuracy: 0.6414\n",
            "10000/10000 [==============================] - 1s 111us/step\n",
            "Model: Random Normal added. Resulting score: 0.6417999863624573\n",
            "\n",
            " ===== Train model: Weight init method: Random Uniform  =====\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/9999\n",
            "40000/40000 [==============================] - 5s 130us/step - loss: 1.7172 - accuracy: 0.3873 - val_loss: 2.2469 - val_accuracy: 0.1788\n",
            "Epoch 2/9999\n",
            "40000/40000 [==============================] - 5s 119us/step - loss: 1.3331 - accuracy: 0.5305 - val_loss: 1.5002 - val_accuracy: 0.4638\n",
            "Epoch 3/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 1.1730 - accuracy: 0.5877 - val_loss: 1.2173 - val_accuracy: 0.5700\n",
            "Epoch 4/9999\n",
            "40000/40000 [==============================] - 5s 117us/step - loss: 1.0558 - accuracy: 0.6309 - val_loss: 1.3827 - val_accuracy: 0.5144\n",
            "Epoch 5/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.9690 - accuracy: 0.6636 - val_loss: 1.1520 - val_accuracy: 0.5910\n",
            "Epoch 6/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.8894 - accuracy: 0.6923 - val_loss: 1.0412 - val_accuracy: 0.6289\n",
            "Epoch 7/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.8238 - accuracy: 0.7171 - val_loss: 1.0957 - val_accuracy: 0.6109\n",
            "Epoch 8/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.7647 - accuracy: 0.7388 - val_loss: 0.9877 - val_accuracy: 0.6511\n",
            "Epoch 9/9999\n",
            "40000/40000 [==============================] - 5s 118us/step - loss: 0.7073 - accuracy: 0.7612 - val_loss: 0.9623 - val_accuracy: 0.6670\n",
            "Epoch 10/9999\n",
            "40000/40000 [==============================] - 5s 120us/step - loss: 0.6544 - accuracy: 0.7812 - val_loss: 0.9896 - val_accuracy: 0.6466\n",
            "Epoch 11/9999\n",
            "40000/40000 [==============================] - 5s 119us/step - loss: 0.6024 - accuracy: 0.8014 - val_loss: 1.0616 - val_accuracy: 0.6410\n",
            "Epoch 12/9999\n",
            "40000/40000 [==============================] - 5s 118us/step - loss: 0.5516 - accuracy: 0.8197 - val_loss: 0.9713 - val_accuracy: 0.6673\n",
            "10000/10000 [==============================] - 1s 119us/step\n",
            "Model: Random Uniform added. Resulting score: 0.6614000201225281\n",
            "\n",
            " ===== Train model: Weight init method: Identity  =====\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/9999\n",
            "40000/40000 [==============================] - 5s 131us/step - loss: 1.6959 - accuracy: 0.3970 - val_loss: 2.2788 - val_accuracy: 0.1091\n",
            "Epoch 2/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 1.3240 - accuracy: 0.5330 - val_loss: 1.4551 - val_accuracy: 0.4923\n",
            "Epoch 3/9999\n",
            "40000/40000 [==============================] - 5s 116us/step - loss: 1.1607 - accuracy: 0.5953 - val_loss: 1.1816 - val_accuracy: 0.5815\n",
            "Epoch 4/9999\n",
            "40000/40000 [==============================] - 5s 122us/step - loss: 1.0498 - accuracy: 0.6373 - val_loss: 1.1595 - val_accuracy: 0.5887\n",
            "Epoch 5/9999\n",
            "40000/40000 [==============================] - 5s 114us/step - loss: 0.9621 - accuracy: 0.6681 - val_loss: 1.1402 - val_accuracy: 0.5927\n",
            "Epoch 6/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.8918 - accuracy: 0.6945 - val_loss: 1.0280 - val_accuracy: 0.6310\n",
            "Epoch 7/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.8245 - accuracy: 0.7190 - val_loss: 1.0228 - val_accuracy: 0.6340\n",
            "Epoch 8/9999\n",
            "40000/40000 [==============================] - 4s 111us/step - loss: 0.7655 - accuracy: 0.7388 - val_loss: 1.0371 - val_accuracy: 0.6338\n",
            "Epoch 9/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.7067 - accuracy: 0.7615 - val_loss: 1.0018 - val_accuracy: 0.6460\n",
            "Epoch 10/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.6525 - accuracy: 0.7817 - val_loss: 1.0720 - val_accuracy: 0.6278\n",
            "Epoch 11/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.6023 - accuracy: 0.8017 - val_loss: 1.0014 - val_accuracy: 0.6494\n",
            "Epoch 12/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 0.5525 - accuracy: 0.8197 - val_loss: 0.9813 - val_accuracy: 0.6556\n",
            "Epoch 13/9999\n",
            "40000/40000 [==============================] - 5s 116us/step - loss: 0.5030 - accuracy: 0.8407 - val_loss: 0.9957 - val_accuracy: 0.6638\n",
            "Epoch 14/9999\n",
            "40000/40000 [==============================] - 5s 114us/step - loss: 0.4553 - accuracy: 0.8573 - val_loss: 1.0406 - val_accuracy: 0.6530\n",
            "Epoch 15/9999\n",
            "40000/40000 [==============================] - 5s 117us/step - loss: 0.4131 - accuracy: 0.8730 - val_loss: 1.0199 - val_accuracy: 0.6599\n",
            "10000/10000 [==============================] - 1s 132us/step\n",
            "Model: Identity added. Resulting score: 0.669700026512146\n",
            "\n",
            " ===== Train model: Weight init method: Orthogonal  =====\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/9999\n",
            "40000/40000 [==============================] - 5s 134us/step - loss: 1.7125 - accuracy: 0.3890 - val_loss: 2.3378 - val_accuracy: 0.1135\n",
            "Epoch 2/9999\n",
            "40000/40000 [==============================] - 5s 118us/step - loss: 1.3031 - accuracy: 0.5438 - val_loss: 1.4121 - val_accuracy: 0.5065\n",
            "Epoch 3/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 1.1405 - accuracy: 0.6015 - val_loss: 1.1654 - val_accuracy: 0.5829\n",
            "Epoch 4/9999\n",
            "40000/40000 [==============================] - 5s 114us/step - loss: 1.0295 - accuracy: 0.6433 - val_loss: 1.1239 - val_accuracy: 0.6057\n",
            "Epoch 5/9999\n",
            "40000/40000 [==============================] - 5s 116us/step - loss: 0.9433 - accuracy: 0.6745 - val_loss: 1.1794 - val_accuracy: 0.5856\n",
            "Epoch 6/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 0.8696 - accuracy: 0.6998 - val_loss: 1.0854 - val_accuracy: 0.6189\n",
            "Epoch 7/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 0.8048 - accuracy: 0.7242 - val_loss: 1.1255 - val_accuracy: 0.6087\n",
            "Epoch 8/9999\n",
            "40000/40000 [==============================] - 5s 116us/step - loss: 0.7425 - accuracy: 0.7470 - val_loss: 0.9930 - val_accuracy: 0.6515\n",
            "Epoch 9/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 0.6852 - accuracy: 0.7681 - val_loss: 1.0015 - val_accuracy: 0.6429\n",
            "Epoch 10/9999\n",
            "40000/40000 [==============================] - 5s 114us/step - loss: 0.6313 - accuracy: 0.7885 - val_loss: 1.0043 - val_accuracy: 0.6484\n",
            "Epoch 11/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.5775 - accuracy: 0.8094 - val_loss: 0.9596 - val_accuracy: 0.6651\n",
            "Epoch 12/9999\n",
            "40000/40000 [==============================] - 5s 114us/step - loss: 0.5304 - accuracy: 0.8255 - val_loss: 0.9656 - val_accuracy: 0.6678\n",
            "Epoch 13/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.4851 - accuracy: 0.8445 - val_loss: 1.0083 - val_accuracy: 0.6610\n",
            "Epoch 14/9999\n",
            "40000/40000 [==============================] - 5s 117us/step - loss: 0.4376 - accuracy: 0.8618 - val_loss: 0.9951 - val_accuracy: 0.6712\n",
            "10000/10000 [==============================] - 1s 112us/step\n",
            "Model: Orthogonal added. Resulting score: 0.664900004863739\n",
            "\n",
            " ===== Train model: Weight init method: Glorot Normal  =====\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/9999\n",
            "40000/40000 [==============================] - 5s 132us/step - loss: 1.6965 - accuracy: 0.4014 - val_loss: 2.0928 - val_accuracy: 0.2909\n",
            "Epoch 2/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 1.3178 - accuracy: 0.5368 - val_loss: 1.4579 - val_accuracy: 0.4914\n",
            "Epoch 3/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 1.1550 - accuracy: 0.5972 - val_loss: 1.1986 - val_accuracy: 0.5706\n",
            "Epoch 4/9999\n",
            "40000/40000 [==============================] - 5s 117us/step - loss: 1.0448 - accuracy: 0.6364 - val_loss: 1.1136 - val_accuracy: 0.6046\n",
            "Epoch 5/9999\n",
            "40000/40000 [==============================] - 5s 117us/step - loss: 0.9585 - accuracy: 0.6668 - val_loss: 1.0684 - val_accuracy: 0.6170\n",
            "Epoch 6/9999\n",
            "40000/40000 [==============================] - 4s 111us/step - loss: 0.8854 - accuracy: 0.6932 - val_loss: 1.0397 - val_accuracy: 0.6318\n",
            "Epoch 7/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.8217 - accuracy: 0.7186 - val_loss: 1.0589 - val_accuracy: 0.6311\n",
            "Epoch 8/9999\n",
            "40000/40000 [==============================] - 5s 119us/step - loss: 0.7589 - accuracy: 0.7424 - val_loss: 1.0142 - val_accuracy: 0.6488\n",
            "Epoch 9/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 0.7025 - accuracy: 0.7634 - val_loss: 0.9921 - val_accuracy: 0.6547\n",
            "Epoch 10/9999\n",
            "40000/40000 [==============================] - 4s 112us/step - loss: 0.6511 - accuracy: 0.7807 - val_loss: 0.9883 - val_accuracy: 0.6533\n",
            "Epoch 11/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.5991 - accuracy: 0.8001 - val_loss: 1.0659 - val_accuracy: 0.6406\n",
            "Epoch 12/9999\n",
            "40000/40000 [==============================] - 5s 114us/step - loss: 0.5489 - accuracy: 0.8217 - val_loss: 1.0190 - val_accuracy: 0.6564\n",
            "10000/10000 [==============================] - 1s 110us/step\n",
            "Model: Glorot Normal added. Resulting score: 0.6521000266075134\n",
            "\n",
            " ===== Train model: Weight init method: Glorot Uniform  =====\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/9999\n",
            "40000/40000 [==============================] - 5s 130us/step - loss: 1.7065 - accuracy: 0.3912 - val_loss: 2.3854 - val_accuracy: 0.1817\n",
            "Epoch 2/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 1.3158 - accuracy: 0.5396 - val_loss: 1.4662 - val_accuracy: 0.4856\n",
            "Epoch 3/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 1.1574 - accuracy: 0.5983 - val_loss: 1.2629 - val_accuracy: 0.5553\n",
            "Epoch 4/9999\n",
            "40000/40000 [==============================] - 4s 110us/step - loss: 1.0457 - accuracy: 0.6385 - val_loss: 1.1503 - val_accuracy: 0.5898\n",
            "Epoch 5/9999\n",
            "40000/40000 [==============================] - 5s 114us/step - loss: 0.9600 - accuracy: 0.6703 - val_loss: 1.1118 - val_accuracy: 0.6043\n",
            "Epoch 6/9999\n",
            "40000/40000 [==============================] - 4s 111us/step - loss: 0.8853 - accuracy: 0.6977 - val_loss: 1.0846 - val_accuracy: 0.6105\n",
            "Epoch 7/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.8193 - accuracy: 0.7221 - val_loss: 1.0383 - val_accuracy: 0.6301\n",
            "Epoch 8/9999\n",
            "40000/40000 [==============================] - 5s 115us/step - loss: 0.7570 - accuracy: 0.7437 - val_loss: 1.0442 - val_accuracy: 0.6319\n",
            "Epoch 9/9999\n",
            "40000/40000 [==============================] - 5s 113us/step - loss: 0.6992 - accuracy: 0.7646 - val_loss: 1.0165 - val_accuracy: 0.6421\n",
            "Epoch 10/9999\n",
            "40000/40000 [==============================] - 5s 114us/step - loss: 0.6449 - accuracy: 0.7861 - val_loss: 0.9803 - val_accuracy: 0.6547\n",
            "Epoch 11/9999\n",
            "40000/40000 [==============================] - 5s 117us/step - loss: 0.5959 - accuracy: 0.8023 - val_loss: 1.0110 - val_accuracy: 0.6511\n",
            "Epoch 12/9999\n",
            "40000/40000 [==============================] - 4s 110us/step - loss: 0.5471 - accuracy: 0.8234 - val_loss: 0.9507 - val_accuracy: 0.6711\n",
            "Epoch 13/9999\n",
            "40000/40000 [==============================] - 4s 111us/step - loss: 0.4971 - accuracy: 0.8407 - val_loss: 0.9943 - val_accuracy: 0.6601\n",
            "Epoch 14/9999\n",
            "40000/40000 [==============================] - 4s 110us/step - loss: 0.4523 - accuracy: 0.8576 - val_loss: 0.9807 - val_accuracy: 0.6669\n",
            "Epoch 15/9999\n",
            "40000/40000 [==============================] - 4s 110us/step - loss: 0.4098 - accuracy: 0.8741 - val_loss: 1.0061 - val_accuracy: 0.6671\n",
            "10000/10000 [==============================] - 1s 107us/step\n",
            "Model: Glorot Uniform added. Resulting score: 0.6674000024795532\n",
            "\n",
            " ===== Accuracy vs weight init methods =====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>weight_init_method</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.6604</td>\n",
              "      <td>Zero</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.6569</td>\n",
              "      <td>Ones</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.6418</td>\n",
              "      <td>Random Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.6614</td>\n",
              "      <td>Random Uniform</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.6697</td>\n",
              "      <td>Identity</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.6649</td>\n",
              "      <td>Orthogonal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.6521</td>\n",
              "      <td>Glorot Normal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.6674</td>\n",
              "      <td>Glorot Uniform</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Accuracy weight_init_method\n",
              "0    0.6604               Zero\n",
              "1    0.6569               Ones\n",
              "2    0.6418      Random Normal\n",
              "3    0.6614     Random Uniform\n",
              "4    0.6697           Identity\n",
              "5    0.6649         Orthogonal\n",
              "6    0.6521      Glorot Normal\n",
              "7    0.6674     Glorot Uniform"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/8 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ===== Converting Binary classification =====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 8/8 [00:00<00:00, 85.50it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ===== Correlation =====\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Zero</th>\n",
              "      <th>Ones</th>\n",
              "      <th>Random Normal</th>\n",
              "      <th>Random Uniform</th>\n",
              "      <th>Identity</th>\n",
              "      <th>Orthogonal</th>\n",
              "      <th>Glorot Normal</th>\n",
              "      <th>Glorot Uniform</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>Zero</th>\n",
              "      <td>NaN</td>\n",
              "      <td>0.525664</td>\n",
              "      <td>0.490412</td>\n",
              "      <td>0.509620</td>\n",
              "      <td>0.493105</td>\n",
              "      <td>0.503716</td>\n",
              "      <td>0.525583</td>\n",
              "      <td>0.501740</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Ones</th>\n",
              "      <td>0.525664</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.472711</td>\n",
              "      <td>0.484394</td>\n",
              "      <td>0.463870</td>\n",
              "      <td>0.491885</td>\n",
              "      <td>0.548974</td>\n",
              "      <td>0.480991</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Normal</th>\n",
              "      <td>0.490412</td>\n",
              "      <td>0.472711</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.488817</td>\n",
              "      <td>0.459353</td>\n",
              "      <td>0.466004</td>\n",
              "      <td>0.469327</td>\n",
              "      <td>0.487661</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Random Uniform</th>\n",
              "      <td>0.509620</td>\n",
              "      <td>0.484394</td>\n",
              "      <td>0.488817</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.473827</td>\n",
              "      <td>0.525723</td>\n",
              "      <td>0.470273</td>\n",
              "      <td>0.470403</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Identity</th>\n",
              "      <td>0.493105</td>\n",
              "      <td>0.463870</td>\n",
              "      <td>0.459353</td>\n",
              "      <td>0.473827</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.477094</td>\n",
              "      <td>0.490986</td>\n",
              "      <td>0.471333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Orthogonal</th>\n",
              "      <td>0.503716</td>\n",
              "      <td>0.491885</td>\n",
              "      <td>0.466004</td>\n",
              "      <td>0.525723</td>\n",
              "      <td>0.477094</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.495129</td>\n",
              "      <td>0.462452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Glorot Normal</th>\n",
              "      <td>0.525583</td>\n",
              "      <td>0.548974</td>\n",
              "      <td>0.469327</td>\n",
              "      <td>0.470273</td>\n",
              "      <td>0.490986</td>\n",
              "      <td>0.495129</td>\n",
              "      <td>NaN</td>\n",
              "      <td>0.472748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Glorot Uniform</th>\n",
              "      <td>0.501740</td>\n",
              "      <td>0.480991</td>\n",
              "      <td>0.487661</td>\n",
              "      <td>0.470403</td>\n",
              "      <td>0.471333</td>\n",
              "      <td>0.462452</td>\n",
              "      <td>0.472748</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    Zero      Ones  ...  Glorot Normal  Glorot Uniform\n",
              "Zero                 NaN  0.525664  ...       0.525583        0.501740\n",
              "Ones            0.525664       NaN  ...       0.548974        0.480991\n",
              "Random Normal   0.490412  0.472711  ...       0.469327        0.487661\n",
              "Random Uniform  0.509620  0.484394  ...       0.470273        0.470403\n",
              "Identity        0.493105  0.463870  ...       0.490986        0.471333\n",
              "Orthogonal      0.503716  0.491885  ...       0.495129        0.462452\n",
              "Glorot Normal   0.525583  0.548974  ...            NaN        0.472748\n",
              "Glorot Uniform  0.501740  0.480991  ...       0.472748             NaN\n",
              "\n",
              "[8 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Average correlation: 0.4883497872034847\n",
            "\n",
            " ===== Computing ensemble accuracy =====\n",
            "Accuracy of ensemble using hard voting: 0.7221\n",
            "Accuracy of ensemble using soft voting: 0.7319\n",
            "\n",
            " ===== Computing ensemble accuracy =====\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "17lFx7lueajW",
        "colab_type": "code",
        "outputId": "eb360c48-db5b-4f39-d4c1-0ea5dfaae216",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "!zip -r /content/CIFAR10_weight_init.zip /content/CIFAR10_weight_init"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/CIFAR10_weight_init/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/9/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/9/correlation_matrix.csv (deflated 64%)\n",
            "  adding: content/CIFAR10_weight_init/9/accuracy.csv (deflated 31%)\n",
            "  adding: content/CIFAR10_weight_init/9/weights/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/9/weights/weights-Identity.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/9/weights/weights-Orthogonal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/9/weights/weights-Ones.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/9/weights/weights-Glorot Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/9/weights/weights-Zero.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/9/weights/weights-Glorot Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/9/weights/weights-Random Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/9/weights/weights-Random Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/9/history/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/9/history/history-Random Uniform.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/9/history/history-Zero.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/9/history/history-Identity.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/9/history/history-Glorot Normal.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/9/history/history-Glorot Uniform.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/9/history/history-Orthogonal.csv (deflated 45%)\n",
            "  adding: content/CIFAR10_weight_init/9/history/history-Ones.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/9/history/history-Random Normal.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/7/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/7/correlation_matrix.csv (deflated 64%)\n",
            "  adding: content/CIFAR10_weight_init/7/accuracy.csv (deflated 33%)\n",
            "  adding: content/CIFAR10_weight_init/7/weights/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/7/weights/weights-Identity.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/7/weights/weights-Orthogonal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/7/weights/weights-Ones.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/7/weights/weights-Glorot Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/7/weights/weights-Zero.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/7/weights/weights-Glorot Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/7/weights/weights-Random Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/7/weights/weights-Random Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/7/history/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/7/history/history-Random Uniform.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/7/history/history-Zero.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/7/history/history-Identity.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/7/history/history-Glorot Normal.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/7/history/history-Glorot Uniform.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/7/history/history-Orthogonal.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/7/history/history-Ones.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/7/history/history-Random Normal.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/8/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/8/correlation_matrix.csv (deflated 65%)\n",
            "  adding: content/CIFAR10_weight_init/8/accuracy.csv (deflated 32%)\n",
            "  adding: content/CIFAR10_weight_init/8/weights/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/8/weights/weights-Identity.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/8/weights/weights-Orthogonal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/8/weights/weights-Ones.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/8/weights/weights-Glorot Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/8/weights/weights-Zero.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/8/weights/weights-Glorot Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/8/weights/weights-Random Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/8/weights/weights-Random Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/8/history/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/8/history/history-Random Uniform.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/8/history/history-Zero.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/8/history/history-Identity.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/8/history/history-Glorot Normal.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/8/history/history-Glorot Uniform.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/8/history/history-Orthogonal.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/8/history/history-Ones.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/8/history/history-Random Normal.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/2/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/2/correlation_matrix.csv (deflated 65%)\n",
            "  adding: content/CIFAR10_weight_init/2/accuracy.csv (deflated 31%)\n",
            "  adding: content/CIFAR10_weight_init/2/weights/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/2/weights/weights-Identity.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/2/weights/weights-Orthogonal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/2/weights/weights-Ones.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/2/weights/weights-Glorot Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/2/weights/weights-Zero.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/2/weights/weights-Glorot Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/2/weights/weights-Random Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/2/weights/weights-Random Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/2/history/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/2/history/history-Random Uniform.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/2/history/history-Zero.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/2/history/history-Identity.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/2/history/history-Glorot Normal.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/2/history/history-Glorot Uniform.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/2/history/history-Orthogonal.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/2/history/history-Ones.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/2/history/history-Random Normal.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/10/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/10/correlation_matrix.csv (deflated 65%)\n",
            "  adding: content/CIFAR10_weight_init/10/accuracy.csv (deflated 32%)\n",
            "  adding: content/CIFAR10_weight_init/10/weights/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/10/weights/weights-Identity.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/10/weights/weights-Orthogonal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/10/weights/weights-Ones.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/10/weights/weights-Glorot Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/10/weights/weights-Zero.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/10/weights/weights-Glorot Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/10/weights/weights-Random Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/10/weights/weights-Random Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/10/history/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/10/history/history-Random Uniform.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/10/history/history-Zero.csv (deflated 46%)\n",
            "  adding: content/CIFAR10_weight_init/10/history/history-Identity.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/10/history/history-Glorot Normal.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/10/history/history-Glorot Uniform.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/10/history/history-Orthogonal.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/10/history/history-Ones.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/10/history/history-Random Normal.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/5/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/5/correlation_matrix.csv (deflated 65%)\n",
            "  adding: content/CIFAR10_weight_init/5/accuracy.csv (deflated 33%)\n",
            "  adding: content/CIFAR10_weight_init/5/weights/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/5/weights/weights-Identity.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/5/weights/weights-Orthogonal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/5/weights/weights-Ones.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/5/weights/weights-Glorot Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/5/weights/weights-Zero.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/5/weights/weights-Glorot Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/5/weights/weights-Random Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/5/weights/weights-Random Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/5/history/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/5/history/history-Random Uniform.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/5/history/history-Zero.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/5/history/history-Identity.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/5/history/history-Glorot Normal.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/5/history/history-Glorot Uniform.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/5/history/history-Orthogonal.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/5/history/history-Ones.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/5/history/history-Random Normal.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/6/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/6/correlation_matrix.csv (deflated 64%)\n",
            "  adding: content/CIFAR10_weight_init/6/accuracy.csv (deflated 32%)\n",
            "  adding: content/CIFAR10_weight_init/6/weights/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/6/weights/weights-Identity.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/6/weights/weights-Orthogonal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/6/weights/weights-Ones.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/6/weights/weights-Glorot Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/6/weights/weights-Zero.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/6/weights/weights-Glorot Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/6/weights/weights-Random Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/6/weights/weights-Random Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/6/history/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/6/history/history-Random Uniform.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/6/history/history-Zero.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/6/history/history-Identity.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/6/history/history-Glorot Normal.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/6/history/history-Glorot Uniform.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/6/history/history-Orthogonal.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/6/history/history-Ones.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/6/history/history-Random Normal.csv (deflated 46%)\n",
            "  adding: content/CIFAR10_weight_init/results_.csv (deflated 46%)\n",
            "  adding: content/CIFAR10_weight_init/4/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/4/correlation_matrix.csv (deflated 65%)\n",
            "  adding: content/CIFAR10_weight_init/4/accuracy.csv (deflated 33%)\n",
            "  adding: content/CIFAR10_weight_init/4/weights/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/4/weights/weights-Identity.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/4/weights/weights-Orthogonal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/4/weights/weights-Ones.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/4/weights/weights-Glorot Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/4/weights/weights-Zero.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/4/weights/weights-Glorot Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/4/weights/weights-Random Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/4/weights/weights-Random Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/4/history/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/4/history/history-Random Uniform.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/4/history/history-Zero.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/4/history/history-Identity.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/4/history/history-Glorot Normal.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/4/history/history-Glorot Uniform.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/4/history/history-Orthogonal.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/4/history/history-Ones.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/4/history/history-Random Normal.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/1/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/1/correlation_matrix.csv (deflated 65%)\n",
            "  adding: content/CIFAR10_weight_init/1/accuracy.csv (deflated 34%)\n",
            "  adding: content/CIFAR10_weight_init/1/weights/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/1/weights/weights-Identity.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/1/weights/weights-Orthogonal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/1/weights/weights-Ones.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/1/weights/weights-Glorot Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/1/weights/weights-Zero.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/1/weights/weights-Glorot Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/1/weights/weights-Random Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/1/weights/weights-Random Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/1/history/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/1/history/history-Random Uniform.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/1/history/history-Zero.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/1/history/history-Identity.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/1/history/history-Glorot Normal.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/1/history/history-Glorot Uniform.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/1/history/history-Orthogonal.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/1/history/history-Ones.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/1/history/history-Random Normal.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/3/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/3/correlation_matrix.csv (deflated 64%)\n",
            "  adding: content/CIFAR10_weight_init/3/accuracy.csv (deflated 32%)\n",
            "  adding: content/CIFAR10_weight_init/3/weights/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/3/weights/weights-Identity.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/3/weights/weights-Orthogonal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/3/weights/weights-Ones.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/3/weights/weights-Glorot Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/3/weights/weights-Zero.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/3/weights/weights-Glorot Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/3/weights/weights-Random Uniform.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/3/weights/weights-Random Normal.h5 (deflated 10%)\n",
            "  adding: content/CIFAR10_weight_init/3/history/ (stored 0%)\n",
            "  adding: content/CIFAR10_weight_init/3/history/history-Random Uniform.csv (deflated 49%)\n",
            "  adding: content/CIFAR10_weight_init/3/history/history-Zero.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/3/history/history-Identity.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/3/history/history-Glorot Normal.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/3/history/history-Glorot Uniform.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/3/history/history-Orthogonal.csv (deflated 47%)\n",
            "  adding: content/CIFAR10_weight_init/3/history/history-Ones.csv (deflated 48%)\n",
            "  adding: content/CIFAR10_weight_init/3/history/history-Random Normal.csv (deflated 48%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}