{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "fde69AMuOpox",
    "outputId": "a210f086-75e4-47f6-abf8-9302bd8e817f"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from itertools import count\n",
    "from sklearn.metrics import accuracy_score\n",
    "from keras.datasets import cifar10\n",
    "from keras.applications import NASNetMobile\n",
    "from keras.layers import Dense, Dropout, Flatten, Activation, Input, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.models import Model, Sequential\n",
    "from keras.callbacks import EarlyStopping, CSVLogger\n",
    "from scipy.stats import pearsonr\n",
    "from tqdm import tqdm\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qYrab7qpOppj"
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 128\n",
    "EPOCHS = 9999\n",
    "IMAGE_SIZE = 32\n",
    "NUM_CLASSES = 10\n",
    "NUM_CHANNELS = 3\n",
    "MODEL_NAME = \"CIFAR_baseline\"\n",
    "PATH = \"\"\n",
    "NR_OF_RUNS = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9M4_-IaBOsn"
   },
   "source": [
    "# Set seeds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "g8QvEt97vF52"
   },
   "source": [
    "# Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "XypdmBJROpp9",
    "outputId": "a3240fff-895b-4cab-f271-ee46b04cc4c9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (50000, 32, 32, 3)\n",
      "50000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "\n",
    "#x_train = preprocess(x_train)\n",
    "#x_test = preprocess(x_test)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mo8yHyg-Opqo"
   },
   "outputs": [],
   "source": [
    "# Convert class vectors to binary class matrices.\n",
    "y_trainc = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
    "y_testc = keras.utils.to_categorical(y_test, NUM_CLASSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a4SYRuKZaIwb"
   },
   "outputs": [],
   "source": [
    "x_train_full = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train_full /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input((32,32,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base = NASNetMobile(include_top = False, weights = None,input_tensor = inputs)(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv_base.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= Flatten()(conv_base)\n",
    "x= Dense(256,activation='relu')(x)\n",
    "outputs=Dense(10,activation='relu')(x)\n",
    "\n",
    "model = Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(learning_rate = 1e-04)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/9999\n",
      "27904/50000 [===============>..............] - ETA: 8:15 - loss: 3.3954 - accuracy: 0.1166"
     ]
    }
   ],
   "source": [
    "model.fit(x_train,y_trainc,\n",
    "          batch_size = BATCH_SIZE,\n",
    "          epochs = EPOCHS,\n",
    "          validation_data = (x_test,y_testc),\n",
    "          shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gIBGIrlkvOt0"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zLWph6_aOpr2"
   },
   "outputs": [],
   "source": [
    "def CIFARmodel(imsize, num_classes, num_channels):\n",
    "    inputs = Input((imsize,imsize,num_channels))\n",
    "    \n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(inputs)\n",
    "    x = Conv2D(filters=64, kernel_size=(3,3), strides=2)(x)\n",
    "    x = Activation('relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(x)\n",
    "    \n",
    "    x = Conv2D(filters=128, kernel_size=(3,3), activation='relu', strides=2, padding='same')(x)\n",
    "    x = Conv2D(filters=128, kernel_size=(3,3), activation='relu', strides=2, padding='same')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    \n",
    "    x = Conv2D(filters=128, kernel_size=(1,1), activation='relu', padding='valid')(x)\n",
    "    x = Conv2D(filters=10, kernel_size=(1,1),strides=(1,1), padding='valid')(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    outputs = Activation('softmax')(x)\n",
    "    \n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate = 1e-04)\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=['accuracy'])\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TVqdcrD_vQ-Q"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 555
    },
    "colab_type": "code",
    "id": "HjvZqLBJOpsw",
    "outputId": "1926576d-79d5-42bc-a308-1f8c2a60dd39"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ===== Train model: Baseline: Run: 10  =====\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/9999\n",
      "40000/40000 [==============================] - 10s 242us/step - loss: 1.7372 - accuracy: 0.3860 - val_loss: 2.3212 - val_accuracy: 0.1073\n",
      "Epoch 2/9999\n",
      "40000/40000 [==============================] - 9s 228us/step - loss: 1.3311 - accuracy: 0.5338 - val_loss: 1.5128 - val_accuracy: 0.4465\n",
      "Epoch 3/9999\n",
      "40000/40000 [==============================] - 9s 228us/step - loss: 1.1638 - accuracy: 0.5937 - val_loss: 1.2326 - val_accuracy: 0.5612\n",
      "Epoch 4/9999\n",
      "40000/40000 [==============================] - 9s 228us/step - loss: 1.0454 - accuracy: 0.6373 - val_loss: 1.1171 - val_accuracy: 0.5995\n",
      "Epoch 5/9999\n",
      "40000/40000 [==============================] - 9s 229us/step - loss: 0.9573 - accuracy: 0.6720 - val_loss: 1.1868 - val_accuracy: 0.5807\n",
      "Epoch 6/9999\n",
      "40000/40000 [==============================] - 9s 229us/step - loss: 0.8789 - accuracy: 0.6974 - val_loss: 1.0727 - val_accuracy: 0.6263\n",
      "Epoch 7/9999\n",
      "40000/40000 [==============================] - 9s 228us/step - loss: 0.8134 - accuracy: 0.7231 - val_loss: 1.0232 - val_accuracy: 0.6369\n",
      "Epoch 8/9999\n",
      "40000/40000 [==============================] - 9s 229us/step - loss: 0.7503 - accuracy: 0.7470 - val_loss: 1.0243 - val_accuracy: 0.6427\n",
      "Epoch 9/9999\n",
      "40000/40000 [==============================] - 9s 229us/step - loss: 0.6926 - accuracy: 0.7685 - val_loss: 0.9951 - val_accuracy: 0.6451\n",
      "Epoch 10/9999\n",
      "40000/40000 [==============================] - 9s 229us/step - loss: 0.6380 - accuracy: 0.7894 - val_loss: 1.0218 - val_accuracy: 0.6414\n",
      "Epoch 11/9999\n",
      "40000/40000 [==============================] - 9s 229us/step - loss: 0.5848 - accuracy: 0.8088 - val_loss: 1.0188 - val_accuracy: 0.6466\n",
      "Epoch 12/9999\n",
      "40000/40000 [==============================] - 9s 230us/step - loss: 0.5385 - accuracy: 0.8257 - val_loss: 0.9851 - val_accuracy: 0.6590\n",
      "10000/10000 [==============================] - 2s 160us/step\n",
      "Run: 10 added. Resulting score: 0.6692000031471252\n",
      "\n",
      " ===== Saving results =====\n"
     ]
    }
   ],
   "source": [
    "for run in range(1, NR_OF_RUNS+1):\n",
    "    # Split the data\n",
    "    x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_trainc, test_size=0.20, shuffle= True)\n",
    "    \n",
    "    models = []\n",
    "    accuracies = []\n",
    "    predictions = []\n",
    "    print(f\"\\n ===== Train model: Baseline: Run: {run}  =====\")\n",
    "        \n",
    "    # Set the seeds\n",
    "    np.random.seed(run*31)\n",
    "    tf.random.set_seed(run*31)\n",
    "\n",
    "    # Create directories\n",
    "    os.makedirs(PATH + MODEL_NAME + f\"/{run}/history\", exist_ok=True)\n",
    "    os.makedirs(PATH + MODEL_NAME + f\"/{run}/weights\", exist_ok=True)\n",
    "        \n",
    "    # weight init method\n",
    "    model = CIFARmodel(IMAGE_SIZE,NUM_CLASSES,NUM_CHANNELS)\n",
    "            \n",
    "    #save weights \n",
    "    weights_path = PATH + MODEL_NAME + f\"/{run}/weights/weights-baseline.h5\"\n",
    "            \n",
    "    if os.path.exists(weights_path):\n",
    "        print(f\"Skipping training of model: weights exists\")\n",
    "        model.load_weights(weights_path)\n",
    "    else:\n",
    "        # initiate early stopping\n",
    "        es = EarlyStopping(min_delta=0.01, patience=3)\n",
    "        csv_logger = CSVLogger(PATH + MODEL_NAME + f\"/{run}/history/history.csv\", separator=';')\n",
    "        # train\n",
    "        model.fit(x_train,y_train,\n",
    "                batch_size = BATCH_SIZE,\n",
    "                epochs = EPOCHS,\n",
    "                validation_data = (x_val,y_val),\n",
    "                shuffle = True,\n",
    "                callbacks=[es, csv_logger])\n",
    "        model.save_weights(weights_path)\n",
    "            \n",
    "    acc = model.evaluate(x_test,y_testc)[1]\n",
    "    print(f\"Run: {run} added. Resulting score: {acc}\")\n",
    "\n",
    "    print(\"\\n ===== Saving results =====\")  \n",
    "    # Save the results\n",
    "    file = PATH + MODEL_NAME + f\"/results_.csv\"\n",
    "    df = pd.DataFrame([[run,acc]])\n",
    "    if not os.path.isfile(file):\n",
    "        df.to_csv(file, header=[\"run\",\"accuracy\"], index=False)\n",
    "    else: # else it exists so append without writing the header\n",
    "        df.to_csv(file, mode='a', header=False, index=False)\n",
    "    clear_output(wait=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fGT6jV-hcLbJ"
   },
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "51rnX_l7Pfdr"
   },
   "source": [
    "## Accuracy\n",
    "The final accuracy of the ensamble on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 920
    },
    "colab_type": "code",
    "id": "GwNmmvSFPlVx",
    "outputId": "b0001421-a29d-427d-bb5d-43e9552531b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: content/CIFAR_baseline/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/9/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/9/weights/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/9/weights/weights-baseline.h5 (deflated 10%)\n",
      "  adding: content/CIFAR_baseline/9/history/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/9/history/history.csv (deflated 47%)\n",
      "  adding: content/CIFAR_baseline/7/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/7/weights/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/7/weights/weights-baseline.h5 (deflated 10%)\n",
      "  adding: content/CIFAR_baseline/7/history/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/7/history/history.csv (deflated 48%)\n",
      "  adding: content/CIFAR_baseline/8/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/8/weights/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/8/weights/weights-baseline.h5 (deflated 10%)\n",
      "  adding: content/CIFAR_baseline/8/history/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/8/history/history.csv (deflated 49%)\n",
      "  adding: content/CIFAR_baseline/2/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/2/weights/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/2/weights/weights-baseline.h5 (deflated 10%)\n",
      "  adding: content/CIFAR_baseline/2/history/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/2/history/history.csv (deflated 47%)\n",
      "  adding: content/CIFAR_baseline/10/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/10/weights/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/10/weights/weights-baseline.h5 (deflated 10%)\n",
      "  adding: content/CIFAR_baseline/10/history/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/10/history/history.csv (deflated 48%)\n",
      "  adding: content/CIFAR_baseline/5/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/5/weights/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/5/weights/weights-baseline.h5 (deflated 10%)\n",
      "  adding: content/CIFAR_baseline/5/history/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/5/history/history.csv (deflated 48%)\n",
      "  adding: content/CIFAR_baseline/6/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/6/weights/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/6/weights/weights-baseline.h5 (deflated 10%)\n",
      "  adding: content/CIFAR_baseline/6/history/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/6/history/history.csv (deflated 47%)\n",
      "  adding: content/CIFAR_baseline/results_.csv (deflated 44%)\n",
      "  adding: content/CIFAR_baseline/4/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/4/weights/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/4/weights/weights-baseline.h5 (deflated 10%)\n",
      "  adding: content/CIFAR_baseline/4/history/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/4/history/history.csv (deflated 48%)\n",
      "  adding: content/CIFAR_baseline/1/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/1/weights/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/1/weights/weights-baseline.h5 (deflated 10%)\n",
      "  adding: content/CIFAR_baseline/1/history/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/1/history/history.csv (deflated 48%)\n",
      "  adding: content/CIFAR_baseline/3/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/3/weights/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/3/weights/weights-baseline.h5 (deflated 10%)\n",
      "  adding: content/CIFAR_baseline/3/history/ (stored 0%)\n",
      "  adding: content/CIFAR_baseline/3/history/history.csv (deflated 47%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r /content/CIFAR_baseline.zip /content/CIFAR_baseline"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "baselineCIFAR10.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
