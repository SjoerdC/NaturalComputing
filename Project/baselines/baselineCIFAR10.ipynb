{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "baselineCIFAR10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fde69AMuOpox",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a210f086-75e4-47f6-abf8-9302bd8e817f"
      },
      "source": [
        "import keras\n",
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from itertools import count\n",
        "from sklearn.metrics import accuracy_score\n",
        "from keras.datasets import cifar10\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.layers import Dense, Dropout, Flatten, Activation, Input, Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Model\n",
        "from keras.callbacks import EarlyStopping, CSVLogger\n",
        "from scipy.stats import pearsonr\n",
        "from tqdm import tqdm\n",
        "from IPython.display import clear_output"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qYrab7qpOppj",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 128\n",
        "EPOCHS = 9999\n",
        "IMAGE_SIZE = 32\n",
        "NUM_CLASSES = 10\n",
        "NUM_CHANNELS = 3\n",
        "MODEL_NAME = \"CIFAR_baseline\"\n",
        "PATH = \"\"\n",
        "NR_OF_RUNS = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R9M4_-IaBOsn",
        "colab_type": "text"
      },
      "source": [
        "# Set seeds"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8QvEt97vF52",
        "colab_type": "text"
      },
      "source": [
        "# Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XypdmBJROpp9",
        "outputId": "a3240fff-895b-4cab-f271-ee46b04cc4c9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "\n",
        "#x_train = preprocess(x_train)\n",
        "#x_test = preprocess(x_test)\n",
        "\n",
        "print('x_train shape:', x_train.shape)\n",
        "print(x_train.shape[0], 'train samples')\n",
        "print(x_test.shape[0], 'test samples')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 11s 0us/step\n",
            "x_train shape: (50000, 32, 32, 3)\n",
            "50000 train samples\n",
            "10000 test samples\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mo8yHyg-Opqo",
        "colab": {}
      },
      "source": [
        "# Convert class vectors to binary class matrices.\n",
        "y_trainc = keras.utils.to_categorical(y_train, NUM_CLASSES)\n",
        "y_testc = keras.utils.to_categorical(y_test, NUM_CLASSES)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4SYRuKZaIwb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_train_full = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train_full /= 255\n",
        "x_test /= 255"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIBGIrlkvOt0",
        "colab_type": "text"
      },
      "source": [
        "# Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zLWph6_aOpr2",
        "colab": {}
      },
      "source": [
        "def CIFARmodel(imsize, num_classes, num_channels):\n",
        "    inputs = Input((imsize,imsize,num_channels))\n",
        "    \n",
        "    x = Conv2D(filters=64, kernel_size=(3,3), activation='relu')(inputs)\n",
        "    x = Conv2D(filters=64, kernel_size=(3,3), strides=2)(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = MaxPooling2D(pool_size=(2,2), strides=(2,2), padding='same')(x)\n",
        "    \n",
        "    x = Conv2D(filters=128, kernel_size=(3,3), activation='relu', strides=2, padding='same')(x)\n",
        "    x = Conv2D(filters=128, kernel_size=(3,3), activation='relu', strides=2, padding='same')(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    \n",
        "    x = Conv2D(filters=128, kernel_size=(1,1), activation='relu', padding='valid')(x)\n",
        "    x = Conv2D(filters=10, kernel_size=(1,1),strides=(1,1), padding='valid')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    outputs = Activation('softmax')(x)\n",
        "    \n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    \n",
        "    optimizer = keras.optimizers.Adam(learning_rate = 1e-04)\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                      optimizer=optimizer,\n",
        "                      metrics=['accuracy'])\n",
        "    return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TVqdcrD_vQ-Q",
        "colab_type": "text"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HjvZqLBJOpsw",
        "outputId": "1926576d-79d5-42bc-a308-1f8c2a60dd39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 555
        }
      },
      "source": [
        "for run in range(1, NR_OF_RUNS+1):\n",
        "    # Split the data\n",
        "    x_train, x_val, y_train, y_val = train_test_split(x_train_full, y_trainc, test_size=0.20, shuffle= True)\n",
        "    \n",
        "    models = []\n",
        "    accuracies = []\n",
        "    predictions = []\n",
        "    print(f\"\\n ===== Train model: Baseline: Run: {run}  =====\")\n",
        "        \n",
        "    # Set the seeds\n",
        "    np.random.seed(run*31)\n",
        "    tf.random.set_seed(run*31)\n",
        "\n",
        "    # Create directories\n",
        "    os.makedirs(PATH + MODEL_NAME + f\"/{run}/history\", exist_ok=True)\n",
        "    os.makedirs(PATH + MODEL_NAME + f\"/{run}/weights\", exist_ok=True)\n",
        "        \n",
        "    # weight init method\n",
        "    model = CIFARmodel(IMAGE_SIZE,NUM_CLASSES,NUM_CHANNELS)\n",
        "            \n",
        "    #save weights \n",
        "    weights_path = PATH + MODEL_NAME + f\"/{run}/weights/weights-baseline.h5\"\n",
        "            \n",
        "    if os.path.exists(weights_path):\n",
        "        print(f\"Skipping training of model: weights exists\")\n",
        "        model.load_weights(weights_path)\n",
        "    else:\n",
        "        # initiate early stopping\n",
        "        es = EarlyStopping(min_delta=0.01, patience=3)\n",
        "        csv_logger = CSVLogger(PATH + MODEL_NAME + f\"/{run}/history/history.csv\", separator=';')\n",
        "        # train\n",
        "        model.fit(x_train,y_train,\n",
        "                batch_size = BATCH_SIZE,\n",
        "                epochs = EPOCHS,\n",
        "                validation_data = (x_val,y_val),\n",
        "                shuffle = True,\n",
        "                callbacks=[es, csv_logger])\n",
        "        model.save_weights(weights_path)\n",
        "            \n",
        "    acc = model.evaluate(x_test,y_testc)[1]\n",
        "    print(f\"Run: {run} added. Resulting score: {acc}\")\n",
        "\n",
        "    print(\"\\n ===== Saving results =====\")  \n",
        "    # Save the results\n",
        "    file = PATH + MODEL_NAME + f\"/results_.csv\"\n",
        "    df = pd.DataFrame([[run,acc]])\n",
        "    if not os.path.isfile(file):\n",
        "        df.to_csv(file, header=[\"run\",\"accuracy\"], index=False)\n",
        "    else: # else it exists so append without writing the header\n",
        "        df.to_csv(file, mode='a', header=False, index=False)\n",
        "    clear_output(wait=True)\n",
        "\n"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            " ===== Train model: Baseline: Run: 10  =====\n",
            "Train on 40000 samples, validate on 10000 samples\n",
            "Epoch 1/9999\n",
            "40000/40000 [==============================] - 10s 242us/step - loss: 1.7372 - accuracy: 0.3860 - val_loss: 2.3212 - val_accuracy: 0.1073\n",
            "Epoch 2/9999\n",
            "40000/40000 [==============================] - 9s 228us/step - loss: 1.3311 - accuracy: 0.5338 - val_loss: 1.5128 - val_accuracy: 0.4465\n",
            "Epoch 3/9999\n",
            "40000/40000 [==============================] - 9s 228us/step - loss: 1.1638 - accuracy: 0.5937 - val_loss: 1.2326 - val_accuracy: 0.5612\n",
            "Epoch 4/9999\n",
            "40000/40000 [==============================] - 9s 228us/step - loss: 1.0454 - accuracy: 0.6373 - val_loss: 1.1171 - val_accuracy: 0.5995\n",
            "Epoch 5/9999\n",
            "40000/40000 [==============================] - 9s 229us/step - loss: 0.9573 - accuracy: 0.6720 - val_loss: 1.1868 - val_accuracy: 0.5807\n",
            "Epoch 6/9999\n",
            "40000/40000 [==============================] - 9s 229us/step - loss: 0.8789 - accuracy: 0.6974 - val_loss: 1.0727 - val_accuracy: 0.6263\n",
            "Epoch 7/9999\n",
            "40000/40000 [==============================] - 9s 228us/step - loss: 0.8134 - accuracy: 0.7231 - val_loss: 1.0232 - val_accuracy: 0.6369\n",
            "Epoch 8/9999\n",
            "40000/40000 [==============================] - 9s 229us/step - loss: 0.7503 - accuracy: 0.7470 - val_loss: 1.0243 - val_accuracy: 0.6427\n",
            "Epoch 9/9999\n",
            "40000/40000 [==============================] - 9s 229us/step - loss: 0.6926 - accuracy: 0.7685 - val_loss: 0.9951 - val_accuracy: 0.6451\n",
            "Epoch 10/9999\n",
            "40000/40000 [==============================] - 9s 229us/step - loss: 0.6380 - accuracy: 0.7894 - val_loss: 1.0218 - val_accuracy: 0.6414\n",
            "Epoch 11/9999\n",
            "40000/40000 [==============================] - 9s 229us/step - loss: 0.5848 - accuracy: 0.8088 - val_loss: 1.0188 - val_accuracy: 0.6466\n",
            "Epoch 12/9999\n",
            "40000/40000 [==============================] - 9s 230us/step - loss: 0.5385 - accuracy: 0.8257 - val_loss: 0.9851 - val_accuracy: 0.6590\n",
            "10000/10000 [==============================] - 2s 160us/step\n",
            "Run: 10 added. Resulting score: 0.6692000031471252\n",
            "\n",
            " ===== Saving results =====\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fGT6jV-hcLbJ",
        "colab_type": "text"
      },
      "source": [
        "# Results"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51rnX_l7Pfdr",
        "colab_type": "text"
      },
      "source": [
        "## Accuracy\n",
        "The final accuracy of the ensamble on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwNmmvSFPlVx",
        "colab_type": "code",
        "outputId": "b0001421-a29d-427d-bb5d-43e9552531b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 920
        }
      },
      "source": [
        "!zip -r /content/CIFAR_baseline.zip /content/CIFAR_baseline"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  adding: content/CIFAR_baseline/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/9/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/9/weights/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/9/weights/weights-baseline.h5 (deflated 10%)\n",
            "  adding: content/CIFAR_baseline/9/history/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/9/history/history.csv (deflated 47%)\n",
            "  adding: content/CIFAR_baseline/7/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/7/weights/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/7/weights/weights-baseline.h5 (deflated 10%)\n",
            "  adding: content/CIFAR_baseline/7/history/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/7/history/history.csv (deflated 48%)\n",
            "  adding: content/CIFAR_baseline/8/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/8/weights/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/8/weights/weights-baseline.h5 (deflated 10%)\n",
            "  adding: content/CIFAR_baseline/8/history/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/8/history/history.csv (deflated 49%)\n",
            "  adding: content/CIFAR_baseline/2/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/2/weights/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/2/weights/weights-baseline.h5 (deflated 10%)\n",
            "  adding: content/CIFAR_baseline/2/history/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/2/history/history.csv (deflated 47%)\n",
            "  adding: content/CIFAR_baseline/10/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/10/weights/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/10/weights/weights-baseline.h5 (deflated 10%)\n",
            "  adding: content/CIFAR_baseline/10/history/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/10/history/history.csv (deflated 48%)\n",
            "  adding: content/CIFAR_baseline/5/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/5/weights/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/5/weights/weights-baseline.h5 (deflated 10%)\n",
            "  adding: content/CIFAR_baseline/5/history/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/5/history/history.csv (deflated 48%)\n",
            "  adding: content/CIFAR_baseline/6/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/6/weights/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/6/weights/weights-baseline.h5 (deflated 10%)\n",
            "  adding: content/CIFAR_baseline/6/history/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/6/history/history.csv (deflated 47%)\n",
            "  adding: content/CIFAR_baseline/results_.csv (deflated 44%)\n",
            "  adding: content/CIFAR_baseline/4/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/4/weights/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/4/weights/weights-baseline.h5 (deflated 10%)\n",
            "  adding: content/CIFAR_baseline/4/history/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/4/history/history.csv (deflated 48%)\n",
            "  adding: content/CIFAR_baseline/1/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/1/weights/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/1/weights/weights-baseline.h5 (deflated 10%)\n",
            "  adding: content/CIFAR_baseline/1/history/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/1/history/history.csv (deflated 48%)\n",
            "  adding: content/CIFAR_baseline/3/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/3/weights/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/3/weights/weights-baseline.h5 (deflated 10%)\n",
            "  adding: content/CIFAR_baseline/3/history/ (stored 0%)\n",
            "  adding: content/CIFAR_baseline/3/history/history.csv (deflated 47%)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}