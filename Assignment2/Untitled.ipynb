{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "\n",
    "#Set seed to make random numbers more predictable for reproducability\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_ad1 = np.random.uniform(low=-1.0, high=1.0, size=(400,2)) \n",
    "classes_ad1 = []\n",
    "for point in points_ad1:\n",
    "    if point[0] >= 0.7:\n",
    "        classes_ad1.append(1)\n",
    "    elif point[0] >= 0.3 and point[1] >= (-0.2-point[0]):\n",
    "        classes_ad1.append(1)\n",
    "    else: \n",
    "        classes_ad1.append(0)\n",
    "classes_ad1 = np.array(classes_ad1)\n",
    "num_clusters_ad1 = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Iris Dataset\n",
    "iris = datasets.load_iris()\n",
    "points_iris = iris.data\n",
    "classes_iris = iris.target\n",
    "num_clusters_iris = 3\n",
    "del iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DEBUG: p1 and p2 should be list\n",
    "def euclidian_distance(p1,p2):\n",
    "    dist = 0\n",
    "    for i in range(len(p1)):\n",
    "        dist += (p1[i] - p2[i])**2\n",
    "    return np.sqrt(dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_to_cluster(points,centroids):\n",
    "    class_assgn = []\n",
    "    num_clusters= len(centroids)\n",
    "    for point in points:\n",
    "        distances = []\n",
    "        for i in range(num_clusters):\n",
    "            distances.append(euclidian_distance(centroids[i],point))\n",
    "        class_assgn.append(np.argmin(distances))\n",
    "    return np.array(class_assgn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recalculate_centroids(classes,points,num_clusters,centroid_old):\n",
    "    centroids = []\n",
    "    for i in range(num_clusters):\n",
    "        indices = np.where(np.array(classes) == i)[0]\n",
    "        sum_ = np.zeros(len(points[0]))\n",
    "        if len(indices) > 0:\n",
    "            for index in indices:\n",
    "                sum_ = sum_ + points[index]\n",
    "            sum_ = sum_ / len(indices)\n",
    "            centroids.append(sum_)\n",
    "        else:\n",
    "            centroids.append(centroid_old[i])\n",
    "    return np.array(centroids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_means_clustering(points,num_clusters,max_iter=1000):\n",
    "    picks = np.random.randint(0,len(points)-1,size=num_clusters)\n",
    "    centroids = []\n",
    "    for pick in picks:\n",
    "        centroids.append(points[pick])\n",
    "    centroids = np.array(centroids)\n",
    "    classes = 0\n",
    "    for i in range(max_iter):\n",
    "        classes = assign_to_cluster(points,centroids)\n",
    "        centroids = recalculate_centroids(classes,points,num_clusters,centroids)\n",
    "    return classes, centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantization_error(points,classes,num_clusters,centroids):\n",
    "    q_e = 0.0\n",
    "    for i in range(num_clusters):\n",
    "        indices = np.where(classes == i)[0]\n",
    "        num_points = len(indices)\n",
    "        for index in indices:\n",
    "            q_e += euclidian_distance(points[index],centroids[i]) / num_points\n",
    "    return q_e/num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the distance between data vectors within a cluster\n",
    "def intracluster_distance(points,classes,num_clusters):\n",
    "    intra_d = 0.0\n",
    "    for i in range(num_clusters):\n",
    "        indices = np.where(np.array(classes) == i)[0]\n",
    "        num_points = len(indices)\n",
    "        for j in range(0,num_points):\n",
    "            for k in range(j+1,num_points):\n",
    "                intra_d += euclidian_distance(points[indices[j]],points[indices[k]])\n",
    "    return intra_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the distance between the centroids of the clusters\n",
    "def intercluster_distance(centroids):\n",
    "    inter_d = 0.0\n",
    "    for i in range(0,len(centroids)):\n",
    "        for j in range(i+1,len(centroids)):\n",
    "            inter_d += euclidian_distance(centroids[i],centroids[j])\n",
    "    return inter_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PSO(points,num_clusters,num_particles=10,funcs=1000,w=0.72,alpha_1 = 1.49,alpha_2 = 1.49):\n",
    "   #For each iteration, num_particles function evaluations are done\n",
    "    dim = len(points[0])\n",
    "    max_iter = funcs//num_particles\n",
    "    #Initialize particles by randomly picking centroids\n",
    "    particles = []\n",
    "    #Velocities initialized to 0\n",
    "    v = np.zeros((num_particles,num_clusters,dim))\n",
    "    for i in range(num_particles):\n",
    "        picks = np.random.randint(0,len(points)-1,size=num_clusters)\n",
    "        centroids = []\n",
    "        for pick in picks:\n",
    "            centroids.append(points[pick])\n",
    "        centroids = np.array(centroids)\n",
    "        particles.append(centroids)\n",
    "    particles = np.array(particles)\n",
    "    #Create local and global best\n",
    "    global_best = 0\n",
    "    global_best_centroid = np.zeros((num_clusters,dim))\n",
    "    local_best = np.zeros(num_particles)\n",
    "    local_best_centroid = np.zeros((num_particles,num_clusters,dim))\n",
    "    #Iterate\n",
    "    for i in range(max_iter):\n",
    "    #Compute fitness of each particle\n",
    "        fitness = []\n",
    "        #for each particle\n",
    "        for j in range(num_particles):\n",
    "            centroids = particles[j]\n",
    "            classes = assign_to_cluster(points,centroids)\n",
    "            this_fitness = quantization_error(points,classes,num_clusters,centroids)\n",
    "            fitness.append(this_fitness)\n",
    "            #Update local best \n",
    "            if (this_fitness < local_best[j]) or (i == 0):\n",
    "                local_best[j] = this_fitness\n",
    "                local_best_centroid[j] = particles[j]\n",
    "        fitness = np.array(fitness)\n",
    "        #Update global best\n",
    "        #Initialize if first iteration\n",
    "        if (np.min(fitness) < global_best) or (i == 0):\n",
    "            global_best = np.min(fitness)\n",
    "            global_best_centroid = particles[np.argmin(fitness)] \n",
    "        #Update particles\n",
    "        for j in range(num_particles):\n",
    "            #Pick r's\n",
    "            r1 = np.random.uniform(low=0, high=1, size=dim)\n",
    "            r2 = np.random.uniform(low=0, high=1, size=dim)\n",
    "            #Update velocity\n",
    "            v[j] = w*v[j] + alpha_1 * np.multiply(r1,local_best_centroid[j]-particles[j]) + alpha_2 * np.multiply(r2,global_best_centroid-particles[j])\n",
    "            #Update particle\n",
    "            particles[j] = particles[j] + v[j]\n",
    "    return assign_to_cluster(points,global_best_centroid),global_best_centroid,particles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run:  0\n",
      "Run:  1\n",
      "Run:  2\n",
      "Run:  3\n",
      "Run:  4\n",
      "Run:  5\n",
      "Run:  6\n",
      "Run:  7\n",
      "Run:  8\n",
      "Run:  9\n",
      "Run:  10\n",
      "Run:  11\n",
      "Run:  12\n",
      "Run:  13\n",
      "Run:  14\n",
      "Run:  15\n",
      "Run:  16\n",
      "Run:  17\n",
      "Run:  18\n",
      "Run:  19\n",
      "Run:  20\n",
      "Run:  21\n",
      "Run:  22\n",
      "Run:  23\n",
      "Run:  24\n",
      "Run:  25\n",
      "Run:  26\n",
      "Run:  27\n",
      "Run:  28\n",
      "Run:  29\n",
      "QE PSO AD1:  0.43485798611378335  +/-  0.14004763014407692\n",
      "QE KMC AD1:  0.5976439442804202  +/-  0.01082149129150552\n",
      "QE PSO Iris:  0.6525096623790099  +/-  0.26914456158345285\n",
      "QE KMC Iris:  0.6620700467311235  +/-  0.026058896906473037\n",
      "Intercluster Distance PSO AD1:  4.87074727277494  +/-  3.4019938867879094\n",
      "Intercluster Distance KMC AD1:  1.0206523773682432  +/-  0.03262531473440729\n",
      "Intercluster Distance PSO Iris:  17.515538921803877  +/-  25.55294598073476\n",
      "Intercluster Distance KMC Iris:  9.761233244386414  +/-  0.6312712780909244\n",
      "Intracluster Distance PSO AD1:  80367.12197584643  +/-  12217.239369347\n",
      "Intracluster Distance KMC AD1:  32530.036440823842  +/-  473.82070518087176\n",
      "Intracluster Distance PSO Iris:  10182.46575446155  +/-  7226.420489574718\n",
      "Intracluster Distance KMC Iris:  4407.1796349856395  +/-  1492.9242796717833\n"
     ]
    }
   ],
   "source": [
    "#Average over executions\n",
    "qe_pso_ad1 = []\n",
    "qe_kmc_ad1 = []\n",
    "qe_pso_iris = []\n",
    "qe_kmc_iris = []\n",
    "\n",
    "inter_pso_ad1 = []\n",
    "inter_kmc_ad1 = []\n",
    "inter_pso_iris = []\n",
    "inter_kmc_iris = []\n",
    "\n",
    "intra_pso_ad1 = []\n",
    "intra_kmc_ad1 = []\n",
    "intra_pso_iris = []\n",
    "intra_kmc_iris = []\n",
    "\n",
    "for i in range(30):\n",
    "    print(\"Run: \",i)\n",
    "    classes_PSO_ad1, centroids_PSO_ad1, particles_ad1 = PSO(points_ad1,2)  \n",
    "    classes_PSO_iris, centroids_PSO_iris, particles_iris = PSO(points_iris,3)  \n",
    "    classes_kmc_ad1,centroids_kmc_ad1 = k_means_clustering(points_ad1,2)\n",
    "    classes_kmc_iris,centroids_kmc_iris = k_means_clustering(points_iris,3)\n",
    "    qe_pso_ad1.append(quantization_error(points_ad1,classes_PSO_ad1,2,centroids_PSO_ad1))\n",
    "    qe_kmc_ad1.append(quantization_error(points_ad1,classes_kmc_ad1,2,centroids_kmc_ad1))\n",
    "    qe_pso_iris.append(quantization_error(points_iris,classes_PSO_iris,3,centroids_PSO_iris))\n",
    "    qe_kmc_iris.append(quantization_error(points_iris,classes_kmc_iris,3,centroids_kmc_iris))\n",
    "    inter_pso_ad1.append(intercluster_distance(centroids_PSO_ad1))\n",
    "    inter_kmc_ad1.append(intercluster_distance(centroids_kmc_ad1))\n",
    "    inter_pso_iris.append(intercluster_distance(centroids_PSO_iris))\n",
    "    inter_kmc_iris.append(intercluster_distance(centroids_kmc_iris))\n",
    "    intra_pso_ad1.append(intracluster_distance(points_ad1,classes_PSO_ad1,2))\n",
    "    intra_kmc_ad1.append(intracluster_distance(points_ad1,classes_kmc_ad1,2))\n",
    "    intra_pso_iris.append(intracluster_distance(points_iris,classes_PSO_iris,3))\n",
    "    intra_kmc_iris.append(intracluster_distance(points_iris,classes_kmc_iris,3))\n",
    "print(\"QE PSO AD1: \", np.mean(qe_pso_ad1), \" +/- \", np.std(qe_pso_ad1))\n",
    "print(\"QE KMC AD1: \",np.mean(qe_kmc_ad1), \" +/- \", np.std(qe_kmc_ad1))\n",
    "print(\"QE PSO Iris: \",np.mean(qe_pso_iris), \" +/- \", np.std(qe_pso_iris))\n",
    "print(\"QE KMC Iris: \",np.mean(qe_kmc_iris), \" +/- \", np.std(qe_kmc_iris))\n",
    "\n",
    "print(\"Intercluster Distance PSO AD1: \", np.mean(inter_pso_ad1), \" +/- \", np.std(inter_pso_ad1))\n",
    "print(\"Intercluster Distance KMC AD1: \",np.mean(inter_kmc_ad1), \" +/- \", np.std(inter_kmc_ad1))\n",
    "print(\"Intercluster Distance PSO Iris: \",np.mean(inter_pso_iris), \" +/- \", np.std(inter_pso_iris))\n",
    "print(\"Intercluster Distance KMC Iris: \",np.mean(inter_kmc_iris), \" +/- \", np.std(inter_kmc_iris))\n",
    "\n",
    "print(\"Intracluster Distance PSO AD1: \", np.mean(intra_pso_ad1), \" +/- \", np.std(intra_pso_ad1))\n",
    "print(\"Intracluster Distance KMC AD1: \",np.mean(intra_kmc_ad1), \" +/- \", np.std(intra_kmc_ad1))\n",
    "print(\"Intracluster Distance PSO Iris: \",np.mean(intra_pso_iris), \" +/- \", np.std(intra_pso_iris))\n",
    "print(\"Intracluster Distance KMC Iris: \",np.mean(intra_kmc_iris), \" +/- \", np.std(intra_kmc_iris))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html\n",
    "\n",
    "#Change this to visualize iris\n",
    "classes = -1\n",
    "\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.decomposition import PCA\n",
    "fig = plt.figure(1, figsize=(8, 6))\n",
    "ax = Axes3D(fig, elev=-150, azim=110)\n",
    "X_reduced = PCA(n_components=3).fit_transform(points_iris)\n",
    "ax.scatter(X_reduced[:, 0], X_reduced[:, 1], X_reduced[:, 2], c=classes,\n",
    "           cmap=plt.cm.Set1, edgecolor='k', s=40)\n",
    "ax.set_title(\"First three PCA directions\")\n",
    "ax.set_xlabel(\"1st eigenvector\")\n",
    "ax.w_xaxis.set_ticklabels([])\n",
    "ax.set_ylabel(\"2nd eigenvector\")\n",
    "ax.w_yaxis.set_ticklabels([])\n",
    "ax.set_zlabel(\"3rd eigenvector\")\n",
    "ax.w_zaxis.set_ticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change this to visualize AD1\n",
    "classes = -1\n",
    "\n",
    "plt.scatter(points_ad1[:,0], points_ad1[:, 1], c=classes, cmap=\"Paired\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' Uncomment below if you would like to run the algorithms only\n",
    "classes_PSO_ad1, centroids_PSO_ad1, particles_ad1 = PSO(points_ad1,2)  \n",
    "classes_PSO_iris, centroids_PSO_iris, particles_iris = PSO(points_iris,3)  \n",
    "classes_kmc_ad1,centroids_kmc_ad1 = k_means_clustering(points_ad1,2)\n",
    "classes_kmc_iris,centroids_kmc_iris = k_means_clustering(points_iris,3)\n",
    "print(\"QE PSO AD1: \",quantization_error(points_ad1,classes_PSO_ad1,2,centroids_PSO_ad1))\n",
    "print(\"QE KMC AD1: \",quantization_error(points_ad1,classes_kmc_ad1,2,centroids_kmc_ad1))\n",
    "print(\"QE PSO Iris: \",quantization_error(points_iris,classes_PSO_iris,3,centroids_PSO_iris))\n",
    "print(\"QE KMC Iris: \",quantization_error(points_iris,classes_kmc_iris,3,centroids_kmc_iris))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
